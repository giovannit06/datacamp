---
title: "Import Data with R"
author: "GT"
date: "6 novembre 2016"
output: html_document
---

## Part 1

### Importing data from flat file...

Lots of data comes in the form of flat files: simple tabular text files.
Learn how to import all common formats of flat file data with base R
functions.

#### read.csv

The `utils` package, which is automatically loaded in you R session
on startup, can import CSV files with the `read.csv()` function.

```{r eval=FALSE}
# Import swimming_pools.csv
pools <- read.csv("swimming_pools.csv")

# Print the structure of pools
str(pools)
```

#### stringAsFactors

With `stringAsFactors`, you can tell R whether it should convert
strings in the flat file to factors. 

For all importing functions in the `utils` package, this argument is
`TRUE`, which means that you import strings as factors. This only
makes sense if the strings you import represent categorical variables
in R. If you set `stringAsFactors` to `FALSE`, the data frame columns
corresponding to strings in your text file will be `character`.

```{r eval=FALSE}
# Import swimming_pools.csv correctly: pools
pools <- read.csv("swimming_pools.csv", stringsAsFactors = FALSE)

# Check the structure of pools
str(pools)
```

#### read.delim

Aside from `.csv` files, there are also the `.txt` files which are
basically text files. You can import these functions with `read.delim()`.
By default, it sets the `sep` argument to `"\t"` (fields in a record
are delimited by tabs) and the `header` argument to `TRUE` (the first
row contains the field names).

```{r eval=FALSE}
# Import hotdogs.txt: hotdogs
hotdogs <- read.delim("hotdogs.txt", header = FALSE)

# Summarize hotdogs
summary(hotdogs)
```

#### read.table

If you're dealing with more exotic flat file formats, you'll want to use
`read.table()`. It's the most basic importing function; you can specify
tons of different arguments in this function. Unlike `read.csv()` and
`read.delim()`, the `header` argument defaults to `FALSE` and the `sep`
argument is `""` by default.

```{r eval=FALSE}
# Path to the hotdogs.txt file: path
path <- file.path("data", "hotdogs.txt")

# Import the hotdogs.txt file: hotdogs
hotdogs <- read.table(path, 
                      sep = "\t", 
                      col.names = c("type", "calories", "sodium"))

# Call head() on hotdogs
head(hotdogs)
```

#### Arguments

You can specify the column names of the data frame.

```{r eval=FALSE}
hotdogs <- read.delim("hotdogs.txt", 
                      header = FALSE, 
                      col.names = c("type",
                                    "calories",
                                    "sodium"))

# Selct the hot dog with the leas calories: lily
lily <- hotdogs[which.min(hotdogs$calories),]
```


#### Column classes

Next to column names, you can also specify the column types or column
classes of the resulting data frame. You can do this by setting the 
`colClasses` argument to a vector of strings representing classes:

```
read.delim("my_file.txt",
            colClasses = c("character",
                            "numeric",
                            "logical"))
```

This approach can be useful if you have some columns that should be
factors and others that should be characters. You don't have to bother
with `stringAsFactors` anymore; just state for each column what the 
class should be.

If a column is set to `"NULL"` in the `colClasses` vector, this
column will be skipped and will not be loaded into the data frame.

```{r eval=FALSE}
# Import the data correctly: hotdogs2
hotdogs2 <- read.delim("hotdogs.txt", header = FALSE, 
                       col.names = c("type", "calories", "sodium"),
                       colClasses = c("factor", "NULL", "numeric"))

# Display structure of hotdogs2
str(hotdogs2)
```

### readr & data.table

Next to base R, there are also dedicated packages to easily and efficiently
import flat file data. We'll talk about two such packages: readr and data.table.

#### read_csv

CSV files can be imported with `read_csv()`. It's a wrapper function around
`read_delim()` that handles all the details for you. For example, it will
assume that the first row contains the column names.

```{r eval=FALSE}
# Load the readr package
library(readr)

# Import potatoes.csv with read_csv(): potatoes
potatoes <- read_csv("potatoes.csv")
```

#### read_tsv

Where you use `read_csv()` to easily read in CSV files, you use `read_tsv()`
to easily read in TSV files. TSV is short for tab-separated values.

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import potatoes.txt: potatoes
potatoes <- read_tsv("potatoes.txt", col_names = properties)

# Call head() on potatoes
head(potatoes)
```

#### read_delim

Just as `read.table()` was the main `utils` function, `read_delim()` is the
main `readr` function.

`read_delim()` takes two mandatory arguments:

- `file`: the file that contains the data
- `delim`: the character that separates the values in the data file

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import potatoes.txt using read_delim(): potatoes
potatoes <- read_delim("potatoes.txt", delim = "\t", col_names = properties)
```

#### skip and n_max

Through `skip` and `n_max` you can control which part of your flat file
you're actually importing into R.

- `skip` specifies the number of lines you're ignoring in the flat file
  before actually starting to import data.
- `n_max` specifies the number of lines you're actually import.

Say for example you have a CSV file with 20 lines, and set `skip = 2` and
`n_max = 3`, you're only reading in lines 3, 4 and 5 of the file.

Watch Out: Once you `skip` some lines, you also skip the first line that
can contain column names!

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import 5 observations from potatoes.txt: potatoes_fragment
# Import observations 7, 8, 9, 10 and 11
potatoes_fragment <- read_tsv("potatoes.txt", skip = 6, n_max = 5, 
                              col_names = properties)
```

#### col_types

You can also specify which types the columns in your imported data frame
should have. You can do this with `col_types`. If set to `NULL`, the
default, functions from the `readr` package will try to find the correct
types themeselves. You can manually set the types with a string, where
each character denotes the class of the column:

- `c`haracter
- `d`ouble
- `i`nteger
- `l`ogical
- `_` skips the column as a whole.

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import all data, but force all columns to be character: potatoes_char
potatoes_char <- read_tsv("potatoes.txt", col_types = "cccccccc", col_names = properties)

# Print out structure of potatoes_char
str(potatoes_char)
```

#### col_types with collectors

Another way of setting the types of the imported columns is using
**collectors**. Collector functions can be passed in a `list()` to
the `col_types` argument of `read_` functions to tell them how to
interpret values in a column.

For a complete list of collector functions, you can take a look at 
the `collector` documentation. For this exercise you will need
two collector functions:

- `col_integer()`: the column should be interpreted as an integer.
- `col_factor(levels, ordered = FALSE`): the column should be 
  interpreted as a factor with `levels`.
  
  
```{r eval=FALSE}
# readr is already loaded

# The collectors you will need to import the data
fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))
int <- col_integer()

# Edit the col_types argument to import the data correctly: hotdogs_factor
hotdogs_factor <- read_tsv("hotdogs.txt",
                           col_names = c("type", "calories", "sodium"),
                           col_types = list(fac, int, int))

# Display the summary of hotdogs_factor
summary(hotdogs_factor)
```

#### fread

You still remember how to use `read.table()`, right? Well, `fread()`
is a function that does the same job withg very similar argument.
It is extremely easy to use and blazingly fast! Often, simply specifying
the path to the file is enough to successfully import your data.

```{r eval=FALSE}
# load the data.table package
library(data.table)

# Import potatoes.csv with fread(): potatoes
potatoes <- fread("potatoes.csv")

# Print out potatoes
potatoes
```

#### fread: more advanced use

Now that you know the basic about `fread()`, you should know about
two arguments of the function `drop` and `select` to drop or select
variables of interest.

Suppose you have a dataset that contains 5 variables and you want 
to keep the first and fifth varible, named "a" and "e". The following
options will all do the trick:

```
fread("path/to/file.txt", drop = 2:4)
fread("path/to/file.txt", select = c(1,5))
fread("path/to/file.txt", drop = c("b", "c", "d"))
fread("path/to/file.txt", select = c("a", "e"))
```
```{r eval=FALSE}
# fread is already loaded

# Import columns 6 and 8 of potatoes.csv: potatoes
potatoes <- fread("potatoes.csv", select = c(6, 8))

# Plot texture (x) and moistness (y) of potatoes
plot(potatoes$texture, potatoes$moistness)
```

#### Dedicated classes

You might have noticed that the `fread()` function produces data
frames that look slightly different when you print them out.
That's because another class named `data.table` is assigned to
the resulting data frames. The printout of such `data.table` objects
is different. 

The class of the result of `fread()` is both `data.table` and
`data.frame`. `read_tsv()` creates an object with three classes:
`tbl_df`, `tbl` and `data.frame`.

What's the benefit of these additional classes? Well, it allows
for a different treatment of printouts, for example.

### Importing Excel data

Excel is very widely used data analysis tool. If you prefer to
do your analyses in R, though, you'll need an understanding of
importing CSV data into R. This chapter will show you how to use
readxl and gdata to do so.

#### List the sheets of an Excel file

Before you can start importing from Excel, you should find
out which sheets are available in the workbook. You can
use the `excel_sheets()` function for this.

```{r eval=FALSE}
# Load the readxl package
library(readxl)

# Print out the names of both spreadsheets
excel_sheets("urbanpop.xlsx")
```

`excel_sheets()` is a simple character vector; you haven't
imported anything yet.

#### Import an Excel sheet

Now that you know the names of the sheets in the Excel file
you want to import, it is time to import those  sheets into R.
You can do this with the `read_excel()` function. Have a look
at this recipe:

```
data <- read_excel("data.xlsx", sheet = "my_sheet")
```

This call simply imports the sheet with the name `"my_sheet"` 
from the `"data_xlsx"` file. You can also pass a number to
the `sheet` argument; this will cause `read_excel()` to
import the sheet with the given sheet number. `sheet = 1`
will import the first sheet, `sheet = 1` will import the
second sheet, and so on.

```{r eval=FALSE}
# The readxl package is already loaded

# Read the sheets, one by one
pop_1 <- read_excel("urbanpop.xlsx", sheet = 1)
pop_2 <- read_excel("urbanpop.xlsx", sheet = 2)
pop_3 <- read_excel("urbanpop.xlsx", sheet = 3)

# Put pop_1, pop_2 and pop_3 in a list: pop_list
pop_list <- list(pop_1, pop_2, pop_3)

# Display the structure of pop_list
str(pop_list)
```

#### Reading a workbook

In the previous exercise you generated a list of three Excel
sheets that you imported. However, loadin in every sheet
manually and then merging them in a list can be quite tedious.
Luckily, you can automate this with `lapply()`.

How a look at the example code below:

```
my_workbook <- lapply(excel_sheets("data.xlsx"),
                      read_excel,
                      path = "data.xlsx")
```

The `read_excel()` function is called multiple times on the
`"data.xlsx"` file and each sheet is loaded in one afterr the
other. The result is a list of data frames, each data frame 
representing one of the sheets in `data.xlsx`.

```{r eval=FALSE}
# The readxl package is already loaded

# Read all Excel sheets with lapply(): pop_list
pop_list <- lapply(excel_sheets("urbanpop.xlsx"),
                    read_excel,
                    path = "urbanpop.xlsx")

# Display the structure of pop_list
str(pop_list)
```

#### The col_names argument

Apart from `path` and `sheet`, there are several othe arguements you
can specify in `read_excel()`. One of these arguments is called
`col_names`.

By default it is `TRUE`, denoting whether the first row in the Excel
sheets contains the column names. If this is not the case, you
can set `col_names` to `FALSE`. In this case, R will choose column
names for you. You can also choose to set `col_names` to a 
character vector with names for each column. It works exactly
the same as in the `readr` package.

```{r eval=FALSE}
# The readxl package is already loaded

# Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a
pop_a <- read_excel("urbanpop_nonames.xlsx", sheet = 1, col_names = FALSE)

# Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b
cols <- c("country", paste0("year_", 1960:1966))
pop_b <- read_excel("urbanpop_nonames.xlsx", sheet = 1, col_names = cols)

# Print the summary of pop_a
summary(pop_a)

# Print the summary of pop_b
summary(pop_b)
```

It's really crucial to correctly tell R whether your Excel data
contains column names. If you don't, the head of the data frame
you end up with will contain incorrect information...

#### The skip argument

Another argument that can be very useful when reading in Excel files
that are less tidy, is `skip`. With `skip`, you can tell R to ignore
a specified number of rows inside the Excel sheets you're trying to
pull data from. Have a look at this example:

```
read_excel("data.xlsx, skip = 15")
```

In this case, the first 15 rows in the first sheet of `"data.xlsx"` 
are ignored.

If the first row of this sheet contained the column names, this
information will also be ignored by `readxl`. Make sure to set
`col_names` to `FALSE` or manually specify column names in this case!

```{r eval=FALSE}
# The readxl package is already loaded

# Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel
urbanpop_sel <- read_excel("urbanpop.xlsx", sheet = 2, skip = 21, col_names = FALSE)

# Print out the first observation from urbanpop_sel
urbanpop_sel[1,]
```

#### Import a local file

In this part of the chapter you'll learn how to import `.xls` files
using the `gdata` package. Similar to `readxl` package, you can import
sigle Excel sheets to start your analysis in R.

```{r eval=FALSE}
# Load the gdata package
library(gdata)

# Import the second sheet of urbanpop.xls: urban_pop
urban_pop <- read.xls("urbanpop.xls", sheet = "1967-1974")

# Print the first 11 observations using head()
head(urban_pop, n = 11)
```

#### read.xls() wraps around read.table()

Remember how `read.xls()` actually works ? It basically comes down
to two steps: converting the Excel file to a `.csv` file using a Perl
script, and then reading that `.csv` file with the `read.csv()` function
that is loaded by default in R, through the `utils` package. 

This means that all the options that you can specify in `read.csv()`,
can also specified in `read.xls()`.

```{r eval=FALSE}
# The gdata package is alreaded loaded

# Column names for urban_pop
columns <- c("country", paste0("year_", 1967:1974))

# Finish the read.xls call
urban_pop <- read.xls("urbanpop.xls", sheet = 2,
                      skip = 50, header = FALSE, stringsAsFactors = FALSE,
                      col.names = columns)

# Print first 10 observation of urban_pop
head(urban_pop, n = 10)
```

#### Work that Excel data!

Now that you can read in Excel data, let's try to clean and merge it. 
You already used the `cbind()` function. Let's take it one step
further now.

```{r eval=FALSE}
# Add code to import data from all three sheets in urbanpop.xls
path <- "urbanpop.xls"
urban_sheet1 <- read.xls(path, sheet = 1, stringsAsFactors = FALSE)
urban_sheet2 <- read.xls(path, sheet = 2, stringsAsFactors = FALSE)
urban_sheet3 <- read.xls(path, sheet = 3, stringsAsFactors = FALSE)

# Extend the cbind() call to include urban_sheet3: urban
# Make sure the first column for urban_sheet2 and urban_sheet3 are
# removed to avoid duplicate columns.
urban <- cbind(urban_sheet1, urban_sheet2[-1], urban_sheet3[-1])

# Remove all rows with NAs from urban: urban_clean
urban_clean <- na.omit(urban)

# Print out a summary of urban_clean
summary(urban_clean)
```

### Reproducible Excel work with XLConnect

Next to importing data from Excel, you can take things one step further with
XLConnect. Learn all about it and bridge the gap between R and Excel!

#### Connect to a workbook

When working with `XLConnect`, the first step will be to load a
workbook in your R session with `loadWorkbook()`; this function
will build a "bridge" between your Excel file and your R session.

```{r eval=FALSE}
# Load the XLConnect package
library(XLConnect)

# Build connection to urbanpop.xlsx: my_book
my_book <- loadWorkbook("urbanpop.xlsx")

# Print out the class of my_book
class(my_book)
```

#### List and read Excel sheets

Just as `readxl` and `gdata`, you can use `XLConnect` to import
data from Excel file into R.

To list the sheets in an Excel file, use `getSheets()`. To actually
import data from a sheet, you can use `readWorksheet()`. Both
function require an XLConnect workbook object as the first
argument.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# List the sheets in my_book
getSheets(my_book)

# Import the second sheet in my_book
readWorksheet(my_book, sheet = 2)
```

#### Customize readWorksheet

To get a clear overview about `urbanpop.xlsx` without having
to open up the Excel file, you can execute the following code:

```
my_book <- loadWorkbook("urbanpop.xlsx")
sheets <- getSheets(my_book)
all <- lapply(sheets, readWorksheet, object = my_book)
str(all)
```

Suppose we're only interested in urban population data of the
years 1968, 1969 and 1970. The data for these years is in the
columns 3, 4 and 5 of the second sheet. Only selecting these
columns will leave us in the dark about the actual countries
the figures belong to.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Import columns 3, 4, and 5 from second sheet in my_book: urbanpop_sel
urbanpop_sel <- readWorksheet(my_book, sheet = 2, startCol = 3, endCol = 5)

# Import first column from second sheet in my_book: countries
countries <- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1)

# cbind() urbanpop_sel and countries together: selection
selection <- cbind(countries, urbanpop_sel)
```

#### Add worksheet

Where `readxl` and `gdata` were only able to import Excel data,
`XLConnect`'s approach of providing an actual interface to an
Excel file makes it able to edit your Excel files from inside
R. In this step, you'll create a new sheet.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Use getSheets() on my_book
getSheets(my_book)
```

#### Populate worksheet

The first step of creating a sheet is done; let's populate it
with some data now! `summ`, a data frame with some summary
statistics on the two Excel sheets is already coded so you can
take it from there.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Create data frame: summ
sheets <- getSheets(my_book)[1:3]
dims <- sapply(sheets, function(x) dim(readWorksheet(my_book, sheet = x)), USE.NAMES = FALSE)
summ <- data.frame(sheets = sheets,
                   nrows = dims[1, ],
                   ncols = dims[2, ])

# Add data in summ to "data_summary" sheet
writeWorksheet(my_book, summ, sheet = "data_summary")

# Save workbook as summary.xlsx
saveWorkbook(my_book, file = "summary.xlsx")
```

#### Renaming sheets

Come to think of it, `"data_summary"` is not an ideal name. As the
summary of these excel sheets is always data-related, you
simply want to name the sheet `"summary"`.

```{r eval=FALSE}
# my_book is available

# Rename "data_summary" sheet to "summary"
renameSheet(my_book, "data_summary", "summary")

# Print out sheets of my_book
getSheets(my_book)

# Save workbook to "renamed.xlsx"
saveWorkbook(my_book, file = "renamed.xlsx")
```

#### Removing sheets

After presenting the new Excel sheet to your peers, it appears
not everybody is a big fan. Why summarize sheets and store
the info in Excel if all the information is implicitly
available? To hell with it, just remove the entire fourth
sheet!

```{r eval=FALSE}
# Load the XLConnect package
library(XLConnect)

# Build connection to renamed.xlsx: my_book
my_book <- loadWorkbook("renamed.xlsx")

# Remove the fourth sheet
removeSheet(my_book, sheet = "summary")

# Save workbook to "clean.xlsx"
saveWorkbook(my_book, file = "clean.xlsx")
```

## Part 2

### Importing data from databases

Many companies store their information in relational databases.
The R community has also developed R packages to get data from
these architectures. You'll learn how to connect to a database and
how to retrieve data from it.

#### Establish a connection

The first step to import data from a SQL database is creating a
connection to it. You need different packages depending on the
database you want to connect to. All of these packages do this
in a uniform way, as specified in the `DBI` package.

`dbConnect()` creates a connection between your R session and
a SQL database. The first argument has to be a `DBIdriver` object,
that specifies how connections are made and how data is mapped
between R and the database. Specifically for MySQL databases, you
can build such a driver with `RMySQL::MySQL()`.

If the MySQL database is a remote database hosted on a server,
you'll also have to specify the followinng argumements in
`dbConnect()`: `dbname`, `host`, `port`, `user` and `password`.

```{r eval=FALSE}
# Load the DBI package
library(DBI)

# Edit dbConnect() call
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "tweater", 
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                 port = 3306,
                 user = "student",
                 password = "datacamp")
```

#### Inspect the connection

Now that you've successfully created the database connection,
let's have a closer look at it.

```
class(con)
[1] "MySQLConnection"
attr(,"package")
[1] "RMySQL"
```

con is an `MySQLConnection` object.

#### List the database tables

After you've successfully connected to a remote MySQL database,
the next step is to see what tables the database contains. You can
do this with the `dbListTables()` function. As you might remember
from the video, this function requires the connection object as
an input, and outputs a character vector with the table names.

```{r eval=FALSE}
# Build a vector of table names: tables
tables <- dbListTables(con)

# Display structure of tables
str(tables)
```

`dbListTables()` can be very useful to get a first idea about the
contents of your database. Can you guess what kind of information
this database contains?

#### Import users

As you might have guessed by now, the database contains data on a
more tasty version of Twitter, namely Tweater. Users can post 
tweats with short recipes for delicious snacks. People can comment
on these tweats. There are three tables: **users**, **tweats** and
**comments** that have relations among them. Which ones, you ask?

Let's start by importing the data on the users into your R session.
You do this with the `dbReadTable()` function. Simply pass it the
connection object (`con`), followed by the name of the table you
want to import. The resulting object is a standard R data frame.

```{r eval=FALSE}
# Import the users table from tweater: users
users <- dbReadTable(con, "users")

# Print users
users
```

#### Import all tables

Next to the `users`, we're also interested in the `tweats` and
`comments` tables. However, separate `dbReadTable()` call for each
and every one of the tables in your database would mean a lot of
code duplication. Remember about the `lapply()` function? You can
use it again here! 

```{r eval=FALSE}
# Get table names
table_names <- dbListTables(con)

# Import all tables
tables <- lapply(table_names, dbReadTable, conn = con)

# Print out tables
tables
```

Now that you have an R version of all data that is contained in
the database, you can dive a little deeper into the relations
between the different data frames.
#### How do the tables relate ?

### Importing data from databases

### Importing data from the web

### Importing data from the web

### Importing data from statistical software