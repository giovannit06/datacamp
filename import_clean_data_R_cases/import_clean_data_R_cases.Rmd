---
title: "Importing & Cleaning Data in R - Case Studies"
author: "GT"
date: "November 14, 2016"
output: html_document
---

### Ticket Sales Data

Hone your skills by importing and cleaning some wonderfully messy
online ticket sales data.

#### Importing the data

You'll be importing and cleaning four real datasets that are a little
messier than before. Don't worry -- you're up for the challenge!

Your first dataset describes online ticket sales for various events
across the country. It's stored as a Comma-Separated Value (CSV) file
calles `sales.csv`. Let's jump right in!

```{r}
sales <- read.csv("sales.csv", stringsAsFactors = FALSE, row.names = 1)
```

#### Examining the data.

As you know from the course, the first step when preparing to clean
data is to inspect it. Let's refresh your memory on some useful 
functions that can do that:

- `dim()` returns the dimensions of an object
- `head()` displays the first part of an object
- `names()` returns the names associated with an object


```{r}
# View dimensions of sales
dim(sales)

# Inspect first 6 rows of sales
head(sales)

# View column names of sales
names(sales)
```

Notice how the rows appear to represent individual purchases and
the columns contain different pieces of information about each
purchase.

#### Summarizing the data

Luckily, the rows and columns appear to be arranged in a meaningful
way: each row represents an observation and each column a variable,
or piece of information about the observation.

In R, there are a great many tools at your disposal to help get
a feel for your data. Besides the three you used in the previous
exercise, the functions `str()` and `summary()` can be very
helpful. 

The `dplyr` package, introduced previously, offers the `glimpse()`
function, which can also be used for this purpose. 

```{r}
# Look at structure of sales
str(sales)

# View a summary of sales
summary(sales)

# Load dplyr
library(dplyr)

# Get a glimpse of sales
glimpse(sales)
```

Before moving on, scroll to the top of `glimpse()` output. Notice
the firt column, `X`, which appears to just be counting.

#### Removing redundant info

You may have noticed that the first column of data is just a
duplication of the row numbers. Not very useful. Go ahead and 
delete that column.

Remember that `nrow()` and `ncol()` return the number of rows
and columns in a data frame, respectively.

Also, recall that you can use square brackets to subset a data
frame as follows:

```
my_df[1:5,]   # First 5 rows of my_df
my_df[, 4]    # Fourth column of my_df
```

Alternatively, you can remove rows and columns using negative
indices. For example:

```
my_df[-(1:5),]  # Omit first 5 rows of my_df
my_df[, 4]      # Omit fourth column of my_df
```

```{r}
# Remove the first column of sales: sales2
sales2 <- sales[,-1]
```

#### Information not worth keeping

Many of the columns have information that's of no use to us.
For example, the first four columns contain internal codes
representing particular events. The last fifteen columns also
aren't worth keeping: there are too many missing values to make
them worthwhile.

An easy way to get rid of unnecessary columns is to create a 
vector containing the column indices you want to keep, then
subset the data based on that vector using single bracket
subsetting.

```{r}
# Define a vector of column indices: keep
keep <- 5:(ncol(sales2)-15)

# Subset sales2 using keep: sales3
sales3 <- sales2[,keep]
```

#### Separating columns

Some of the columns in your data frame include multiple pieces of
information that should be in separate columns. In this exercise, 
you will separate such a column into two: one for date and one for
time. You will use the `separate()` function from the `tidyr`
package.

Take a look at the `event_date_time` column by typing
`head(sales3$event_date_time)` in the console. You'll notice that
the date and time are separated by a space. Therefore, you'll use
`sep = ""` as un argument to `separate()`.

```{r}
# Load tidyr
library(tidyr)

# Inspect first 6 rows of sales3$event_date_time
head(sales3$event_date_time)

# Split event_date_time: sales4
sales4 <- separate(sales3, event_date_time,
                   c("event_dt", "event_time"), sep = " ")

# Inspect first 6 rows of sales4$sales_ord_create_dttm
head(sales4$sales_ord_create_dttm)

# Split sales_ord_create_dttm: sales5
sales5 <- separate(sales4, sales_ord_create_dttm,
                    c("ord_create_dt", "ord_create_time"), sep = " ")
```

Did you see the warning message that just popped up in the console ?
No need to panic. You'll sort it out in the next exercise.

#### Dealing with warnings 

Looks like the second call to `separate()` threw a warning. Not to worry;
warnings aren't as bas as error messages. It's not saying that the command
didn't execute; it's just a heads-up that something unusual happened.

The warning say `Too few values at 4 locations`. You may be able to guess
already what the issue is, but it's still good to take a look.

The locations (i.e. rows) given in the warning are 2516, 3863, 4082 and 4183.
Have a look at the contents of the `sales_ord_create_dttm` column in
those rows.

```{r}
# Define an issues vector
issues <- c(2516, 3863, 4082, 4183)

# Print values of sales_ord_create_dttm at these indices
sales3$sales_ord_create_dttm[issues]

# Print a well-behaved value of sales_ord_create_dttm
sales3$sales_ord_create_dttm[2517]
```

The warning was just because of four missing values. You'll ignore them
for now, but if your analysis depended on complete date/time information,
you would probabily need to delete those rows.

#### Identifying dates

Some of the columns in your dataset contain dates of different events.
Right now, they are stored as character strings. That's fine if all you
want to do is look up the date associated with an event, but if you want
to do any comparisons or math with the dates, it's MUCH easier to store
them as `Date` objects.

Luckily, all of the date columns in this dataset have the substring `"dt"`
in their name, so you can use the `str_detect()` function of the `stringr`
package to find the date columns. Then you can coerce them to `Date` objects
using a function from the `lubridate` package.

You'll use `lapply()` to apply the appropriate `lubridate` function to
all of the columns that contain dates. Recall the following syntax for
`lapply()` applied to some data frame columns of interest:

```
lapply(my_data_frame[, cols], function_name)
```

Also recall that function names in `lubridate` combine the letters
`y`, `m`, `d`, `h`, `m`, `s` depending on the format of the date/time
string being read in.

```{r}
# Load stringr
library(stringr)

# Find columns of sales5 containing "dt": date_cols
date_cols = str_detect(names(sales5), "dt")

# Load lubridate
library(lubridate)

# Coerce date columns into Date objects
sales5[, date_cols] <- lapply(sales5[,date_cols], ymd)
```

Your code looks great, but there were a few more warnings.

#### More warnings!

As you saw, some of the calls to `ymd()` caused a `failure to parse`
warning. That's probably because of more missing data, but again,
it's good to check to be sure.

The first two lines of code (provided for you here) create a list of
logical vectors called `missing`. Each vector in the list indicates the
presence (or absence) of missing values in the corresponding column of
`sales5`. See if the number of missing values in each column is the
same as the number of rows that failed to parse in the previous exercise.

```{r}
# Find date columns (don't change)
date_cols <- str_detect(names(sales5), "dt")

# Create logical vectors indicating missing values (don't change)
missing <- lapply(sales5[, date_cols], is.na)

# Create a numerical vector that counts missing values: num_missing
num_missing <- sapply(missing, sum)

# Print num_missing
num_missing
```

#### Combining columns

Sure enough, the number of `NA` in each column match the numbers from
the warning messages, so missing data is the culprit. How to proceed
depends on your desider analysis. If you really need complete sets of
date/time information, you might delete the rows or columns containing
`NA`.

As your last step, you'll use the `tidyr` function `unite()` to combine the
`venue_city` and `vanue_state` columns into one column with the two values
separated by a comma and a space. For example, `"PORTLAND"` `"MAINE"` should
become `"PORTLAND, MAINE"`.

```{r}
# Combine the venue_city and venue_state columns
sales6 <- unite(sales5, venue_city_state, venue_city, venue_state, sep = ", ")

# View the head of sales6
head(sales6)
```

This dataset is mucch cleaner. Your next steps would depend on what
specific analyses you wanted to perform.

### MBTA Ridership Data

Boston's public transit system needs your help! The T wants to do some
data analysis and you've been asked to clean their ridership data.

#### Using readx!

The Massachusetts Bay Transportation Authority ("MBTA" or just 
"the T" for short) manages America's oldest subway, as well as
Greater Boston's commuter rail, ferry and bus systems.

It's your first day on the job as the T's data analyst and you've been
tasked with analyzing average ridership through time. You're in luck,
because this chapter of the course will guide you through cleaning
a set of MBTA ridership data!

The dataset is stored as an Excel spreadsheet called `mbta.xlsx` in
your working directory. You'll use the `read_excel()` function from
Hadley Wickham's `readxl` package to import it.

The first time you import a dataset, you might not know how many
rows need to be skipped. In this case, the first row it a title, so
you'll need to skip the first row.

```{r}
# Load readxl
library(readxl)

# Import mbta.xlsx and skip first row: mbta
mbta <- read_excel("mbta.xlsx", skip = 1)
```

#### Examining the dataa

Your new boss at the T has tasked you with analyzing the ridership
data. Of course, you're going to clean the dataset first. The first
step when cleaning a dataset it to explore it a bit.

The `mbta` data frame is already loaded in your workspace. Pay particular
attention to how the rows and columns are organized and to the location
of missing values.

```{r}
# View the structure of mbta
str(mbta)

# View the first 6 rows of mbta
head(mbta)

# View a summary of mbta
summary(mbta)
```

Do you notice anything strange about how the rows and columns are 
organized ?

#### Removing unnecessary rows and columns

It appears that the data are organized with observations stored as
columns rather than as rows. You can fix that.

First, though, you can address the missing data. All of the `NA` values
are storedin the `All Modes by Qtr` row. This row really belongs in a
different data frame; it is a quarterly average of weekday MBTA ridership.
Since this dataset tracks monthly average ridership, you'll remove that
row.

Similarly, the 7th row (`Pct Chg / Yr`) and the 11th row (`TOTAL`) are
not really observations as much as they are analysis.  Go ahead and
remove the 7th and 11th rows as well.

The first column also needs to be removed because it's just listing
the row numbers.

In case you were wondering, this dataset is stored as a `tibble` which
is just a specific type of data frame.

```{r}
# Remove rows 1, 7, and 11 of mbta: mbta2
mbta2 <- mbta[-c(1,7,11),]

# Remove the first column of mbta2: mbta3
mbta3 <- mbta2[,-1]
```

#### Observations are stored in columns

Recall from a few exercises back that in your T ridership data,
variables are stored in rows instead of columns.

The different modes of transportation (commuter rail, bus, subway,
ferry, ...) are **variables**, providing information about each
month's average ridership. The months themselves are **observations**.
You can tell which is which because as you go through time, the
month changes, but the modes of transport offered by the T do not.

As is customary, you want to represent variables in columns
rather than rows. The first step is to uset the `gather()` 
function from the `tidyr` package, which will gather columns
into key-value pairs.

```{r}
# Load tidyr
library(tidyr)

# Gather columns of mbta3: mbta4
mbta4 <- gather(mbta3, month, thou_riders, -mode)

# View the head of mbta4
head(mbta4)
```

Your dataset is **long** now - notice that the first column still
stores variable names , but now, months have their column instead
of being headers. Also notice the data type in each column.

#### Type conversions

In a minute, you'll put variables where they belong (as column
names). But first, take this oppurtunity to change the average
weekday ridership column, `thou_riders`, into numeric values
rather than character strings. That way, you'll be able to do
things like compare values and do math.

```{r}
# Coerce thou_riders to numeric
mbta4$thou_riders <- as.numeric(mbta4$thou_riders)
```

#### Variables are stored in both rows and colums

Now, you can finish the job you started earlier: getting 
variables into columns. Right now, variables are stored as 
"keys" in the `mode` column. You'll use the `tidyr` function
`spread()` to make them into columns containing average
weekday ridership for the given month and mode of transport.

```{r}
# Spread the contents of mbta4: mbta5
mbta5 <- spread(mbta4, mode, thou_riders)

# View the head of mbta5
head(mbta5)
```

#### Separating columns

Your dataset is already looking much better! Your boss saw
what a great job you're doing and now wants you to do an analysis
of the T's ridership during certain months across all years.

Your dataset has month names in it, so that analysis will be a 
piece of cake. There's only one small problem: if you want to
look at ridership on the T during every January (for example),
the month and year are together in the same column, which makes 
it a little tricky.

In this exercise, you'll separate the `month` column into
distinct `month` and `year` columns to make life easier.

```{r}
# View the head of mbta5
head(mbta5)

# Split month column into month and year: mbta6
mbta6 <- separate(mbta5, month, c("year", "month"), sep = "-")

# View the head of mbta6
head(mbta6)
```

#### Do your values seem reasonable ?

Before you write up the analysis for your boss, it's a good idea
to screen the data for any obvious mistakes and/or outliers.

There are many valid techniques for doing this.

```{r}
# View a summary of mbta6
summary(mbta6)

# Generate a histogram of Boat ridership
hist(mbta6$Boat)
```

#### Dealing with entry error

Think for a minute about that `Boat` histogram. Every month,
average weekday coomuter boat ridership was on either side of
four thousand. Then, one month it jumped to 40 thousand without
warning ?

Unless the Oympics were happening in Boston that month, this value
is certainly an error. You can assume that whoever was entering
the data that month accidentally typed `40` instead of `4`.

Because it's error, you don't want this value influencing your
analysis. In this exercise, you'll locate the incorrect value
and change it to `4`.

After you make the change, you'll run the last two commands in
the editor as-is. They use function you may not know yet to produce
some cool ridership plots: one showing the lesser-used modes of
transport (take a look at gorgeous seasonal variation in Boat
ridership), and one showing all modes of transport. The plots are
based on the long version of the data we produced in Exercise 4 --
a good example of using different data formats for different
purposes.

```{r}
# Find the row number of the incorrect value: i
i <- which(mbta6$Boat > 10 )

# Replace the incorrect value with 4
mbta6$Boat[i] <- 4

# Generate a histogram of Boat column
hist(mbta6$Boat)

library(ggplot2)
mbta_boat <- subset(mbta4, mode %in% c("Boat", "Trackless Trolley"))
# Look at Boat and Trackless Trolley ridership over time (don't change)
ggplot(mbta_boat, aes(x = month, y = thou_riders, col = mode)) +  geom_point() + 
  scale_x_discrete(name = "Month", breaks = c(200701, 200801, 200901, 201001, 201101)) + 
  scale_y_continuous(name = "Avg Weekday Ridership (thousands)")
mbta_all <- mbta4
# Look at all T ridership over time (don't change)
ggplot(mbta_all, aes(x = month, y = thou_riders, col = mode)) + geom_point() + 
  scale_x_discrete(name = "Month", breaks = c(200701, 200801, 200901, 201001, 201101)) +  
  scale_y_continuous(name = "Avg Weekday Ridership (thousands)")
```

### World Food Facts

We all know that you are what you eat, so what exactly are you?
In this chapter, you'll import and clean some data about food 
products from around the world.

#### Importing the data

As a person of many talents, it's time to take on a different job:
nutritional analysis! Your goal is to analyze the sugar content
of a sample of foods from around the world.

A large dataset called `food.csv` is ready for your use in the 
working directory. Instead of the usual `read.csv()`, however, you're
going to use the faster `fread()` from the `data.table` package.
The data will come in as a data table, but since you're used
to working with data frames, you can just convert it.

```{r}
# Load data.table
library(data.table)

# Import food.csv: food
food <- fread("food.csv")

# Convert food to a data frame
food <- data.frame(food) 
food <- food[,-1]
dim(food)
```

#### Examining the data

As usual, you'll need to get an idea of what the dataset looks
like in order to know how to proceed.

```{r}
# View summary of food
summary(food)

# View head of food
head(food)

# View structure of food
str(food)
```

With datasets this big, it's hard to get a handle on exactly
what they contain.

#### Inspecting variables

The `str()`, `head()` and `summary()` functions are designed
to give you some information about a dataset without being
overwhelming. However, this dataset is so large and has
so many variables that even these outputs seemed pretty 
intimidating!

The `glimpse()` function from the `dplyr` package often formats
information in a more approacheble way.

Yet another option is to just look at the column names to see
what kinds of data you have. As you look at the names, pay
particular attention to any pairs that look like duplicates.

```{r}
# Load dplyr
library(dplyr)

# View a glimpse of food
glimpse(food)

# View column names of food
names(food)
```

#### Removing duplicate Info

Wow. That's a lot of variables. To summarize, there's some
information on what and when information was added(1:9), meta
information about food (10:17, 22, 27), where it come from
(18:21, 28:34), what it's made of (35:52), nutrition grade (53:54),
som unclear (55:63), and some nutritional information (64:159).

There are also many different pairs of columns that contain
duplicate information. Luckily, you have a trusty assistant 
who went through and identified duplicate columns for you.

A vector has been created for you that lists out all of the
duplicates; all you need to do is remove those columns from
the dataset. Don't forget, you can use the `-` operator to
specify columns to omit, e.g.: 

```
my_df[, -3] # Omit third column
```

```{r}
# Define vector of duplicate cols (don't change)
duplicates <- c(4, 6, 11, 13, 15, 17, 18, 20, 22, 
                24, 25, 28, 32, 34, 36, 38, 40, 
                44, 46, 48, 51, 54, 65, 158)

# Remove duplicates from food: food2
food2 <- food[, -duplicates]
```

#### Removing useless Info

Your dataset is much more manageable already.

In addition to duplicate columns, there are many columns
containing information that you just can't use. For example,
the first few columns contain internal codes that don't have
any meaning to us. There are also some column names that
aren't clear enough to tell what they contain.

All of these columns can be deleted. Once again, your
assistant did a splendid job finding the indices for you.

```{r}
# Define useless vector (don't change)
useless <- c(1, 2, 3, 32:41)

# Remove useless columns from food2: food3
food3 <- food2[, - useless]
```

#### Finding columns

Looking much nicer! Recall from the first exercise that you
are assuming you will be analyzing the sugar content of these
foods. Therefore, your next step is to look at a summary of 
the nutrition information.

All of the columns with nutrition info contain the character
string `"100g"` as part of their name, which makes it easy
to identify them.

```{r}
# Load stringr
library(stringr)

# Create vector of column indices: nutrition
nutrition <- str_detect(names(food3), "100g")

# View a summary of nutrition columns
summary(food3[,nutrition])
```

#### Replacing missing values

Unfortunately, the summary revealed that the nutrition data are
mostly `NA` values. After consulting with the lab technician,
it appears that much of the data is missing because the food
just doesn't have those nutrients.

But all is not lost! The lab tech also said that for sugar
content, zero values are sometimes entered explicitly, but
sometimes the values are just left empty to denote a zero.
A statistical miracle!

In this exercise, you'll replace all `NA` values with zeroes
in the `sugar_100g` column and make histograms to visualize
the result. Then, you will exclude the observations which have
no sugar to see how the distribution changes.

```{r}
# Find indices of sugar NA values: missing
missing <- is.na(food3$sugars_100g)

# Replace NA values with 0
food3$sugars_100g[missing] <- 0

# Create first histogram
hist(food3$sugars_100g)

# Create food4
food4 <- food3[food3$sugars_100g != 0, ]

# Create second histogram
hist(food4$sugars_100g)
```

Excluding the observations which don't contain any sugar, 
you can better visualie what the underlying distribution
looks like.

#### Dealing with messy data

Your analysis of sugar content was so impressive that you've
now been tasked with determining how many of these foods
come in some sort of plastic packaging.

Your dataset has information about packaging, but there's a bit
of a problem: it's stored in several different languages.
This takes messy data to a whole new level! There is no R
package to selectively translate, but what if you could jus
work with the messy data directly ?

You're in luck! The root word for _plastic_ is same in English
(plastic), French (plastique), and Spanish (plastico). To get
a general idea of how many of these foods are packaged in
plastic, you can look through the `packaging-tags` column for
the string `"plasti"`.

```{r}
# Find entries containing "plasti": plastic
plastic <- str_detect(food3$packaging_tags, "plasti")

# Print the sum of plastic
sum(plastic)
```

### School Attendance Data

Use all of the tools you've learned to import and clean a gnarly
dataset containing information on average school attendance in the US.

#### Importing the data

In this chapter, you'll work with attendance data from public
schools in the US, organized by school level and state, during
2007-2008 academic year. The data contain information on
average daily attendance (ADA) as a percentage of total 
enrollment, school day length, and school year length.

The data were given to you in an Excel spreadsheet.

Do you see any symptoms of untidy data ? At first glance, it looks
like the first row is a description of the data, the second row
is a variable itself that groups multiple columns together, and
the fourth row give numbers for the columns, which might look
nice in a spreadsheet but isn't very useful for you, the analyst.

You'll take it one step at a time to import the data using
the `gdata` package. The name of this spreadsheet is 
`attendance.xls`.

```{r}
# Load the gdata package
library(gdata)

# Import the spreadsheet: att
att <- read.xls("attendance.xls")
```

Whoops, looks like your call to `read.xls()` triggered a warning.
You already got experience interpreting warning messages in
previous chapters, so we'll just go ahead and tell you: this
one's nothing to worry about.

#### Examining the data

#### Removing unnecessary rows

#### Removing useless columns

#### Splitting the data

#### Replacing the names

#### Cleaning up extra characters

#### Some final type conversions