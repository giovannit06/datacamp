---
title: "Cleaning Data in R"
author: "GT"
date: "November 8, 2016"
output: html_document
---

### Introduction and exploring raw data

This chapter will give you an overview of the process of data cleaning
with R, then walk you through the basics of exploring raw data.

#### The data cleaning process

1. Exploring raw data
2. Tidying data
3. Preparing data for analysis
4. Putting it all together

No one likes missing data, but it is dangerous to assume that 
it can simply be removed or replaced. Sometimes missing data tells us
something important about whatever it is that we're measuring 
(i.e. The value of the variable that is missing may be related to the
reason it is missing). 
Such data is called Missing not at Random, or MNAR.

#### Here's what messy data look like

In the final chapter of this course, you will be presented with 
a messy, real-world dataset containing an entire year's worth of 
weather data from Boston, USA. Among other things, you'll be presented
with variables that contain column names, column names that should be
values, numbers coded as character strings, and values that are 
missing, extreme, and downright erroneous!

```{r eval=FALSE}
# View the first 6 rows of data
head(weather)

# View the last 6 rows of data
tail(weather)

# View a condensed summary of the data
str(weather)
```

```
> # View the first 6 rows of data
> head(weather)
  X year month           measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14
1 1 2014    12  Max.TemperatureF 64 42 51 43 42 45 38 29 49  48  39  39  42  45
2 2 2014    12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39  43  36  35  37  39
3 3 2014    12  Min.TemperatureF 39 33 37 30 26 38 21 18 29  38  32  31  32  33
4 4 2014    12    Max.Dew.PointF 46 40 49 24 37 45 36 28 49  45  37  28  28  29
5 5 2014    12    MeanDew.PointF 40 27 42 21 25 40 20 16 41  39  31  27  26  27
6 6 2014    12     Min.DewpointF 26 17 24 13 12 36 -3  3 28  37  27  25  24  25
  X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31
1  42  44  49  44  37  36  36  44  47  46  59  50  52  52  41  30  30
2  37  40  45  40  33  32  33  39  45  44  52  44  45  46  36  26  25
3  32  35  41  36  29  27  30  33  42  41  44  37  38  40  30  22  20
4  33  42  46  34  25  30  30  39  45  46  58  31  34  42  26  10   8
5  29  36  41  30  22  24  27  34  42  44  43  29  31  35  20   4   5
6  27  30  32  26  20  20  25  25  37  41  29  28  29  27  10  -6   1
> 
> # View the last 6 rows of data
> tail(weather)
      X year month            measure   X1   X2   X3   X4   X5   X6   X7   X8
281 281 2015    12 Mean.Wind.SpeedMPH    6 <NA> <NA> <NA> <NA> <NA> <NA> <NA>
282 282 2015    12  Max.Gust.SpeedMPH   17 <NA> <NA> <NA> <NA> <NA> <NA> <NA>
283 283 2015    12    PrecipitationIn 0.14 <NA> <NA> <NA> <NA> <NA> <NA> <NA>
284 284 2015    12         CloudCover    7 <NA> <NA> <NA> <NA> <NA> <NA> <NA>
285 285 2015    12             Events Rain <NA> <NA> <NA> <NA> <NA> <NA> <NA>
286 286 2015    12     WindDirDegrees  109 <NA> <NA> <NA> <NA> <NA> <NA> <NA>
      X9  X10  X11  X12  X13  X14  X15  X16  X17  X18  X19  X20  X21  X22  X23
281 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
282 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
283 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
284 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
285 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
286 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
     X24  X25  X26  X27  X28  X29  X30  X31
281 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
282 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
283 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
284 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
285 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
286 <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> 
> # View a condensed summary of the data
> str(weather)
'data.frame':	286 obs. of  35 variables:
 $ X      : int  1 2 3 4 5 6 7 8 9 10 ...
 $ year   : int  2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ...
 $ month  : int  12 12 12 12 12 12 12 12 12 12 ...
 $ measure: chr  "Max.TemperatureF" "Mean.TemperatureF" "Min.TemperatureF" "Max.Dew.PointF" ...
 $ X1     : chr  "64" "52" "39" "46" ...
 $ X2     : chr  "42" "38" "33" "40" ...
 $ X3     : chr  "51" "44" "37" "49" ...
 $ X4     : chr  "43" "37" "30" "24" ...
 $ X5     : chr  "42" "34" "26" "37" ...
 $ X6     : chr  "45" "42" "38" "45" ...
 $ X7     : chr  "38" "30" "21" "36" ...
 $ X8     : chr  "29" "24" "18" "28" ...
 $ X9     : chr  "49" "39" "29" "49" ...
 $ X10    : chr  "48" "43" "38" "45" ...
 $ X11    : chr  "39" "36" "32" "37" ...
 $ X12    : chr  "39" "35" "31" "28" ...
 $ X13    : chr  "42" "37" "32" "28" ...
 $ X14    : chr  "45" "39" "33" "29" ...
 $ X15    : chr  "42" "37" "32" "33" ...
 $ X16    : chr  "44" "40" "35" "42" ...
 $ X17    : chr  "49" "45" "41" "46" ...
 $ X18    : chr  "44" "40" "36" "34" ...
 $ X19    : chr  "37" "33" "29" "25" ...
 $ X20    : chr  "36" "32" "27" "30" ...
 $ X21    : chr  "36" "33" "30" "30" ...
 $ X22    : chr  "44" "39" "33" "39" ...
 $ X23    : chr  "47" "45" "42" "45" ...
 $ X24    : chr  "46" "44" "41" "46" ...
 $ X25    : chr  "59" "52" "44" "58" ...
 $ X26    : chr  "50" "44" "37" "31" ...
 $ X27    : chr  "52" "45" "38" "34" ...
 $ X28    : chr  "52" "46" "40" "42" ...
 $ X29    : chr  "41" "36" "30" "26" ...
 $ X30    : chr  "30" "26" "22" "10" ...
 $ X31    : chr  "30" "25" "20" "8" ...
> 
```

You may have already noticed messy aspects of the dataset. 
If not, don't worry! We'll get to that soon.

#### Here's what clean data look like

In this course, you will acquire many new tools in your data cleaning
toolbox for whipping the weather data into shape!

```{r eval=FALSE}
# View the first 6 rows of data
head(weather_clean)

# View the last 6 rows of data
tail(weather_clean)

# View a condensed summary of the data
str(weather_clean)
```

```
> # View the first 6 rows of data
> head(weather_clean)
        date    events cloud_cover max_dew_point_f max_gust_speed_mph
1 2014-12-01      Rain           6              46                 29
2 2014-12-02 Rain-Snow           7              40                 29
3 2014-12-03      Rain           8              49                 38
4 2014-12-04      None           3              24                 33
5 2014-12-05      Rain           5              37                 26
6 2014-12-06      Rain           8              45                 25
  max_humidity max_sea_level_pressure_in max_temperature_f max_visibility_miles
1           74                     30.45                64                   10
2           92                     30.71                42                   10
3          100                     30.40                51                   10
4           69                     30.56                43                   10
5           85                     30.68                42                   10
6          100                     30.42                45                   10
  max_wind_speed_mph mean_humidity mean_sea_level_pressure_in
1                 22            63                      30.13
2                 24            72                      30.59
3                 29            79                      30.07
4                 25            54                      30.33
5                 22            66                      30.59
6                 22            93                      30.24
  mean_temperature_f mean_visibility_miles mean_wind_speed_mph mean_dew_point_f
1                 52                    10                  13               40
2                 38                     8                  15               27
3                 44                     5                  12               42
4                 37                    10                  12               21
5                 34                    10                  10               25
6                 42                     4                   8               40
  min_dew_point_f min_humidity min_sea_level_pressure_in min_temperature_f
1              26           52                     30.01                39
2              17           51                     30.40                33
3              24           57                     29.87                37
4              13           39                     30.09                30
5              12           47                     30.45                26
6              36           85                     30.16                38
  min_visibility_miles precipitation_in wind_dir_degrees
1                   10             0.01              268
2                    2             0.10               62
3                    1             0.44              254
4                   10             0.00              292
5                    5             0.11               61
6                    0             1.09              313
> 
> # View the last 6 rows of data
> tail(weather_clean)
          date events cloud_cover max_dew_point_f max_gust_speed_mph
361 2015-11-26   None           6              49                 28
362 2015-11-27   None           7              52                 32
363 2015-11-28   Rain           8              50                 23
364 2015-11-29   None           4              33                 20
365 2015-11-30   None           6              26                 17
366 2015-12-01   Rain           7              43                 17
    max_humidity max_sea_level_pressure_in max_temperature_f
361          100                     30.87                59
362          100                     30.63                64
363           93                     30.20                60
364           79                     30.42                44
365           75                     30.53                38
366           96                     30.40                45
    max_visibility_miles max_wind_speed_mph mean_humidity
361                   10                 22            79
362                   10                 26            78
363                   10                 18            80
364                   10                 16            58
365                   10                 14            65
366                   10                 15            83
    mean_sea_level_pressure_in mean_temperature_f mean_visibility_miles
361                      30.77                 49                     9
362                      30.41                 56                     9
363                      30.16                 51                     9
364                      30.26                 38                    10
365                      30.46                 33                    10
366                      30.24                 39                     8
    mean_wind_speed_mph mean_dew_point_f min_dew_point_f min_humidity
361                  10               42              34           57
362                  14               49              47           56
363                  10               43              36           67
364                  10               23              15           36
365                   9               23              18           54
366                   6               35              25           69
    min_sea_level_pressure_in min_temperature_f min_visibility_miles
361                     30.64                38                    5
362                     30.15                48                    5
363                     30.11                41                    4
364                     30.19                32                   10
365                     30.39                28                   10
366                     30.01                32                    1
    precipitation_in wind_dir_degrees
361             0.00              180
362             0.00              209
363             0.21              358
364             0.00              326
365             0.00               65
366             0.14              109
> 
> # View a condensed summary of the data
> str(weather_clean)
'data.frame':	366 obs. of  23 variables:
 $ date                      : POSIXct, format: "2014-12-01" "2014-12-02" ...
 $ events                    : chr  "Rain" "Rain-Snow" "Rain" "None" ...
 $ cloud_cover               : num  6 7 8 3 5 8 6 8 8 8 ...
 $ max_dew_point_f           : num  46 40 49 24 37 45 36 28 49 45 ...
 $ max_gust_speed_mph        : num  29 29 38 33 26 25 32 28 52 29 ...
 $ max_humidity              : num  74 92 100 69 85 100 92 92 100 100 ...
 $ max_sea_level_pressure_in : num  30.4 30.7 30.4 30.6 30.7 ...
 $ max_temperature_f         : num  64 42 51 43 42 45 38 29 49 48 ...
 $ max_visibility_miles      : num  10 10 10 10 10 10 10 10 10 10 ...
 $ max_wind_speed_mph        : num  22 24 29 25 22 22 25 21 38 23 ...
 $ mean_humidity             : num  63 72 79 54 66 93 61 70 93 95 ...
 $ mean_sea_level_pressure_in: num  30.1 30.6 30.1 30.3 30.6 ...
 $ mean_temperature_f        : num  52 38 44 37 34 42 30 24 39 43 ...
 $ mean_visibility_miles     : num  10 8 5 10 10 4 10 8 2 3 ...
 $ mean_wind_speed_mph       : num  13 15 12 12 10 8 15 13 20 13 ...
 $ mean_dew_point_f          : num  40 27 42 21 25 40 20 16 41 39 ...
 $ min_dew_point_f           : num  26 17 24 13 12 36 -3 3 28 37 ...
 $ min_humidity              : num  52 51 57 39 47 85 29 47 86 89 ...
 $ min_sea_level_pressure_in : num  30 30.4 29.9 30.1 30.4 ...
 $ min_temperature_f         : num  39 33 37 30 26 38 21 18 29 38 ...
 $ min_visibility_miles      : num  10 2 1 10 5 0 5 2 1 1 ...
 $ precipitation_in          : num  0.01 0.1 0.44 0 0.11 1.09 0.13 0.03 2.9 0.28 ...
 $ wind_dir_degrees          : num  268 62 254 292 61 313 350 354 38 357 ...
> 
```

Doesn't this dataset already look nicer than in the previous exercise?

#### Getting a feel for your data

The first thing to do when you get your hands on a new dataset is to
understand its structure. There are several ways to go about this
in R, each of which may reveal different issues with your data that
require attention.

In this course, we are only concerned with data that can be
expressed in table format (i.e. two dimensions, rows and columns).
As you may recall from earlier courses, tables in R often have
the type `data.frame`. You can check the `class` of any object
in R with the `class()` function. 

Once you know that you are dealing with tabular data, you may also
wanto to get a quick feel for the contents of your data. Before
printing the entire dataset to the console, it's probabily worth
knowing how many rows and columns there are. The `dim()` command
tells you this.

Loaded a dataset called bmi into your workspace. 
The data, which give the (age standardized) mean body mass 
index (BMI) among males in each country for the years 
1980-2008

```{r}
# Load bmi.csv
bmi <- read.csv("bmi.csv")

# Check the class of bmi
class(bmi)

# Check the dimensions of bmi
dim(bmi)

# View the column names of bmi
names(bmi)
```

#### Viewing the structure of your data

Since `bmi` doesn't have a huge number of columns, you can view
a quick snapshot of your data using `str()` command. In addition
to the class and dimensions of your _entire dataset_, `str()` will
tell you the class of _each variable_ and give you a preview of 
its contents.

Although we won't go into detail on the `dplyr` package in this lesson,
the `glimpse()` function from `dplyr` is a slightly cleaner 
alternative to `str()`. `str()` and `glimpse()` give you a preview 
of your data, which may reveal issues with the way columns are labelled,
how variables are encoded, etc.

You can use the `summary()` command to get a better feel for 
how your data are distributed, which may reveal unusual or extreme
values, unexpected missing data, etc. For numeric variables, this
means looking at means, quartiles (including median), and extreme
values. For character or factor variables, you may be curious
about the number of times each value appears in the data (i.e. counts).

```{r}
# Check the structure of bmi
str(bmi)

# Load dplyr
library(dplyr)

# Check the structure of bmi, the dplyr way
glimpse(bmi)

# View a summary of bmi
summary(bmi)
```

#### Looking at your data

You can look at all the summaries you want, but at the end of the
day, there is no substitute for looking at your data, either in
raw table form or by plotting it.

The most basic way to look at your data in R is by printing it
to the console. As you may know from experience, the `print()`
command is not even necessary; you can just type the name of
the object. The downside to this option is that R will attempt 
to print the entire dataset, which can be a nuisance if the
dataset is too large.

One way around this is to use the `head()` and `tail()` commands,
which only display the first and last 6 rows of data, respectively.
You can view more (or fewer) rows by providing as a second 
argument to the function the number of rows you wish to view.
These functions provide a useful method for quickly getting a sense
of your data without overly cluttering the console.

```{r}
# View the first 6 rows
head(bmi)

# View the first 15 rows
head(bmi, n=15)

# View the last 6 rows
tail(bmi)

# View the last 10 rows
tail(bmi, n=10)
```

#### Visualizing your data

There are many ways to visualize data. Since this is not a course
about data visualization, we will only touch on two types of plots
that may be useful for quickly identifying extreme or suspicious values
in your data: histograms and scatter plots.

A histogram, created with the `hist()` function, takes a vector 
(i.e. column) of data, breaks it up to intervals, then plots as a 
vertical bar the number of instances within each interval.

A scatter plot, created with the `plot()` function, takes two
vectors of data and plots them as a series of `(x, y)` coordinates
on a two-dimensional plane.

 
 ```{r}
# Histogram of BMIs from 2008
hist(bmi$Y2008)

# Scatter plot comparing BMIs from 1980 to those from 2008
plot(bmi$Y1980, bmi$Y2008)
```

### Tidying data

#### Principles of tidy data

- Observation as rows
- Variables as columns
- One type of observational unit per table

#### Common symptoms of messy data

Multiple variables stored in one column is messy, but there should be
multiple values in each column.

#### What kind of messy are the BMI data?

Which symptom of messy data is exhibited by `bmi` ?

Column headers are values, not variable names.
All of the year column names could be expressed as values
of a new variable called `year`.

#### Gathering columns Into key-value pairs

The most important function in `tidyr` is `gather()`. It should
be used when you have columns that are not varibles and you want
to collapse them into key-value pairs.

The easiest way to visualize the effect of `gather()` is that it makes
wide datasets long. As you saw in the video, running the
following command on `wide_df` will make it long:

```
gather(wide_df, my_key, my_val, -col)
```

```{r}
# Load dplyr
library(tidyr)

# Apply gather() to bmi and save the result as bmi_long
bmi_long <- gather(bmi, year, bmi_val, -Country)

# View the first 20 rows of the result
head(bmi_long, n = 20)
```

Notice how now, instead of being represented in the column names, 
years are now all neatly represented in the `year` column. 
Try checking `dim(bmi_long)` and `dim(bmi)` before moving on.

```{r}
dim(bmi_long)
dim(bmi)
```

#### Spreading key-value pairs into columns

The opposite of `gather()` is `spread()`, which takes key-values pairs
and spreads theme across multiple columns. This is useful when values
in a column should actually be column names (i.e. variables). 
It can also make data more compact and easier to read.

The easiest way to visualize the effect of `spread()` is that it makes
long datasets wide. As you saw in the video, running the following
command will make `long_df` wide: 

```
spread(long_df, my_key, my_val)
```

```{r}
# Apply spread() to bmi_long
bmi_wide <- spread(bmi_long, year, bmi_val)

# View the head of bmi_wide
head(bmi_wide)

# Check dimension of bmi_wide and bmi
dim(bmi_wide)
dim(bmi)
```

#### Function in tidyr

- `gather()` - Gather columns into key-value pairs
- `spread()` - Spread key-value pairs into columns
- `separate()` - Separate one column into multiple
- `unite()` - Unite multiple columns into one

#### Separating columns

The `separate()` function allows you to separate one column
into multiple columms. Unless you tell it otherwise, it will
attempt to separate on any character that is not a letter
or number. You can also specify a specific separator using
the `sep` argument. 

For exammple we have a dataset called `treatments`. This dataset
obeys the principle of tidy data, but we'd like to split the 
treatment dates onto two separate columns: `year` and `month`.
This can be accomplished with the following:

```
separate(tratments, years_mo, c("year", "month"))
```

For the dataset called `bmi_cc`, the `Country_ISO` column has
the name of each country as well its two-letter ISO country code,
separated by a forward slash.

```{r eval=FALSE}
# Apply separate() to bmi_cc
bmi_cc_clean <- separate(bmi_cc, col = Country_ISO, into = c("Country", "ISO"), sep = "/")

# Print the head of the result
head(bmi_cc_clean)
```

It's good to be familiar with the separate function. In practice, 
it's often used to break date-time values into their individual 
pieces (day, month, year, etc.)

#### Uniting columns

The opposite of `separate()` is `unite()`, which takes multiple 
columns and pastes them together. By default, the contents of the
columns will be separated by underscores in the new column, but
this behavior can be altered via the `sep` argument.

This time the `year_mo` column has been separated into `year` and
`month`. The original column can be recreated by putting `year`
and `month` back together:

```
unite(treatments, year_mo, year, month)
```

As previously, put the columns `Country` and `ISO` together.

```{r eval=FALSE}
# Apply unite() to bmi_cc_clean
bmi_cc <- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = "-")

# View the head of the result
head(bmi_cc)
```

#### Column headers are values, not variable names

You saw earlier in the chapter how we sometimes come across
datasets where column names are actually values of a variable
(e.g. months of the year). This is often the case when working
with repeated measures data, where measurements are taken on
subjects of intererst on multiple occasions over time. The 
`gather()` function is helpful in these situations.

```{r eval=FALSE}
## tidyr and dplyr are already loaded for you

# View the head of census
head(census)

# Gather the month columns
census2 <- gather(census, month, amount, -YEAR)

# Arrange rows by YEAR using dplyr's arrange
census2 <- arrange(census2, YEAR)

# View first 20 rows of census2
head(census2, n = 20)
```

Many datasets dealing with historical data appear in this format 
with time expressed horizontally. Understanding how to tidy these
data is key for efficient data analysis.

#### Variables are stored in both rows and columns

Sometimes you'll run into situations where varibles are stored
in both rows and columns. To illustrate this, we've loaded the 
`pets` dataset from the video, which tell us in a convoluted way
how many birds, cats and dogs, Jason, Lisa and Terrence have.

```
> pets

     owner type num
1    Jason  dog   2
2    Jason  cat   4
3    Jason bird   3
4     Lisa  dog   7
5     Lisa  cat  10
6     Lisa bird   9
7 Terrence  dog   8
8 Terrence  cat   5
9 Terrence bird   1
```

Altough it may not be immediately obvious,if we treat the values
in the `type` column as variables and create a separate  column
for each of them, we can set things straight. To do this, we use the
`spread()` function. 

```
> spread(pets, type, num)

     owner bird cat dog
1    Jason    3   4   2
2     Lisa    9  10   7
3 Terrence    1   5   8
```

The result shows the exact same information in a much clearer way!
Notice that the `spread()` function took in three arguments.
The first argument takes the name of your messy dataset (`pets`),
the second argument takes the name of the column to spread into
new columns (`type`), and the third argument takes the column that
contains the value with which to fill in the newly spread out 
columns (`num`). 

```{r eval=FALSE}
## tidyr is already loaded for you

# View first 50 rows of census_long
head(census_long, n = 50)

# Spread the type column
census_long2 <- spread(census_long, type, amount);

# View first 20 rows of census_long2
head(census_long2, n = 20)
```

#### Multiple values are stored in one column

It's also fairly common that you will find two variables stored
in a sigle column of data. These variables may be joined by a 
separator like a dash, underscore, space, or forward slash.

The `separate()` function comes in handy in these situations. 
To practice using it, we have created a slight modification of
last exercise's result. Keep in mind that the `into` argument, 
which specifies the names of the 2 new columns being formed, must
be given as a character vector (e.g. `c("column1", "column2")`).

```{r eval=FALSE}
## tidyr is already loaded for you

# View the head of census_long3
head(census_long3)

# Separate the yr_month column into two
census_long4 <- separate(census_long3, yr_month, c("year", "month"), sep = "_")

# View the first 6 rows of the result
head(census_long4)
```

### Preparing data for analysis

This chapter will teach you how to prepare your data for analysis. 
We will look at type conversion, string manipulation, missing and 
special values, and outliers and obvious errors.

#### Types of variables in R

As in other programming languages, R is capable of storing data
in many different formats, most of which you've probably seen by now.

Loosely speaking, the `class()` function tell you what type of object
you're working with. (There are subtle differences between the
`class`, `type` and `mode` of an object, but these distinctions
are beyond the scope of this course.)

```{r}
# Make this evaluate to character
class("true")

# Make this evaluate to numeric
class(8484.00)

# Make this evaluate to integer
class(99L)

# Make this evaluate to factor
class(factor("factor"))

# Make this evaluate to logical
class(FALSE)
```

#### Common type conversions

It is often necessary to change, or _coerce_, the way that
variables in a dataset are stored. This could be because
of the way they were read into R (with `read.csv()`, for example)
or perhaps the function you are using to analyze the data requires
variables to be coded a certain way.

Only certain coercions are allowed, but the rules for what works
are generally pretty intuitive. For example, trying to convert
a character string to a number **gives an error*:

```
as.numeric("some text")

Warning message: NAs introduced by coercion
[1] NA
```

There are a few less intuitive results. For example, under the hood,
the logical values `TRUE` and `FALSE` are coded as `1` and `0`,
respectively. Therefore, `as.logical(1)` returns `TRUE` and
`as.numeric(TRUE)` returns `1`.

```{r eval=FALSE}
# Preview students with str()
str(students)

# Coerce Grades to character
students$Grades <- as.character(students$Grades)

# Coerce Medu to factor
students$Medu <- factor(students$Medu)

# Coerce Fedu to factor
students$Fedu <- factor(students$Fedu)
    
# Look at students once more with str()
str(students)
```

It's important to make sure that you know how to change variable 
types in the event that you get some surprises when you first 
import your data!

#### Working with dates

Dates can be a challenge to work with in any programming language, 
but thanks to the `lubridate` package, working with dates in R 
isn't so bad. Since this course is about cleaning data, we only
cover the most basic functions from `lubridate` to help us standardize
the fromat of dates and times on our data.

These functions, combine letters `y`, `m`, `d`, `h`, `m`, `s`, 
which stand for year, month, day, hour, minute, and second,
respectively. The order of the letters in the function should match
the order of the date/time you are attempting to read in, although
not all combinations are valid. Notice that the functions are
"smart" in that they are capable of parsing multiple formats.

```{r}
# Load the lubridate package
library(lubridate)

# Parse as date
dmy("17 Sep 2015")

# Parse as date and time (with no seconds!)
mdy_hm("July 15, 2012 12:56")
```

#### Trimming and padding strings

One common issue that comes up when cleaning data is the need to
remove leading and/or trailing white space. The `str_trim()` 
function from `stringr` makes it easy to do this while leaving
intact the part of the string that you actually want.

```
> str_trim("   this is a test    ")
[1] "this is a test"
```

A similar issue is when you need to pad strings to make them a 
certain number of characters wide. One example is if you had a 
bunch of employee ID numbers, some of which begin
 with one or more zeros. When reading these data in, you find
 that the leading zeros have been dropped somewhere along the
 way (probabily because the variable was thought to be numeric
 and in that case, leading zeros would be unnecessary).
 
 ```
 > str pad("24493", width = 7, side = "left", pad = "0")
[1] "0024493"
```

```{r}
# Load the stringr package
library(stringr)

# Trim all leading and trailing whitespace
str_trim(c("   Filip ", "Nick  ", " Jonathan"))

# Pad these strings with leading zeros
str_pad(c("23485W", "8823453Q", "994Z"), width = 9, side = "left", pad = "0")
```

Examples like this are certainly handy in R. For example, 
the `str_pad()` function is useful when importing a dataset with 
US zip codes. Occasionally R will drop the leading 0 in a zipcode, 
thinking it's numeric. Now that you know how to coerce variable 
types and pad strings, this won't set you back!

#### Upper and lower case

In addition to trimming and padding strings, you may need to
adjust their case from time to time. Making strings uppercase
or lowercase is very straightforward in (base) R thanks to 
`toupper()` and `tolower()`. Each function takes exactly one 
argument: the character string (or vector/column of strings) to
be converted to the desidered case.

```{r}
states <- c("al", "ak", "az", "ar", "ca", "co", "ct", "de", "fl", 
            "ga", "hi", "id", "il", "in", "ia", "ks", "ky", "la", 
            "me", "md", "ma", "mi", "mn", "ms", "mo", "mt", "ne", 
            "nv", "nh", "nj", "nm", "ny", "nc", "nd", "oh", "ok", 
            "or", "pa", "ri", "sc", "sd", "tn", "tx", "ut", "vt", 
            "va", "wa", "wv", "wi", "wy")
# Print state abbreviations
head(states)

# Make states all uppercase and save result to states_upper
states_upper <- toupper(states)
head(states_upper)

# Make states_upper all lowercase again
tolower(states_upper)
```

#### Finding and replacing strings

The `stringr` package provides two functions that are very useful
for finding and/or replacing strings: `str_detect()` and 
`str_replace()`.

Like all functions in stringr, the first argument of each is the
string of interest. The second argument of each is the pattern
of interest. In the aces of `str_detect()`, this is the pattern
we are searching for. In the case of `str_replace()`, this is the
pattern we want to replace. Finally, `str_replace()` has a third
argument, which is the string to replace with.

```{r echo=FALSE}
X <- 1:6
school <- rep("GB", 6)
sex <- c("F", "F", "F", "F", "F", "M")
dob <- c("2000-06-05", "1999-11-25", "1998-02-02", "1997-12-20",
         "1998-10-04", "1999-06-16")
students2 <- data.frame(X, school, sex, dob)
```


```{r}
## stringr has been loaded for you

# Look at the head of students2
head(students2)

# Detect all dates of birth (dob) in 1997
str_detect(students2$dob, "1997")

# In the sex column, replace "F" with "Female"...
students2$sex <- str_replace(students2$sex, "F", "Female")

# ...And "M" with "Male"
students2$sex <- str_replace(students2$sex, "M", "Male")

# View the head of students2
head(students2)
```

#### Types of missing and special values in R

Missing values

- In R, represented as `NA`
- May appear in other forms, `#N/A` (Excel), single dot (SPSS, SAS)

Special values

- `Inf` - "Infinite value"
- `NaN` - "Not a number"

#### Finding missing values

As you've seen, missing values in R should be represented by `NA`,
but unfortunately you will not always be so lucky. Before you can deal
with missing values, you have to find them in the data.

If missing value are properly coded as `NA`, the `is.na()` function
will help you find them. Otherwise, if your datase is too big to just
look at the whole thing, you may need to try searching for some
of the usual suspects like `""`, `"#N/A"`, etc. You can also use
the `summary()` and `table()` functions to turn up unexpected values
in your data.

```{r echo=FALSE}
name <- c("Sarah", "Tom", "David", "Alice")
n_friends <- c(244, NA, 145, 43)
status <- c("Going out!", "", "Movie night...", "")
social_df <- data.frame(name, n_friends, status)
```

```{r}
# Call is.na() on the full social_df to spot all NAs
is.na(social_df)

# Use the any() function to ask whether there are any NAs in the data
any(is.na(social_df))

# View a summary() of the dataset
summary(social_df)

# Call table() on the status column
table(social_df$status)
```

Scanning your dataset for `NA` values is essential before learning how 
to remedy missing data problems.

#### Dealing with missing values

Missing values can be a rather complex subject, but here we'll only
look at the simple case where you are simply interested in 
normalizing and/or removing all missing values from your data.
For more information on why this is not always the best strategy,
search online for "missing not at random".

Looking at the `social_df` dataset again, we asked around a bit and
figured out what's causing the missing values that you saw in the
last exercise. Tom doesn't have a social media account on this
particular platform, which explain why his number of friends and
current status are missing. Alice is on the platform, but is a passive
user and never sets her status, hence the reason it's missing for her.

```{r}
## The stringr package is preloaded

# Replace all empty strings in status with NA
social_df$status[social_df$status == ""] <- NA

# Print social_df to the console
social_df

# Use complete.cases() to see which rows have no missing values
complete.cases(social_df)

# Use na.omit() to remove all rows with any missing values
na.omit(social_df)
```

Often times in data analyses, you'll want to get a feel for how many
complete observations you have. This can be helpful in determining 
how you handle observations with missing data points.

#### Dealing with outliers and obvious errors

When dealing with strange values in your data, you often must decide
whether they are just extreme or actually erroneous. Extreme values
show up all over the place, but you, the data analyst, must figure
out when they are plausible and when they are not.

```{r eval=FALSE}
# Look at a summary() of students3
summary(students3)

# View a histogram of the age variable
hist(students3$age)

# View a histogram of the absences variable
hist(students3$absences)

# View a histogram of absences, but force zeros to be bucketed to the right of zero
hist(students3$absences, right = FALSE)
```

#### Another look at strange values

Another useful way of looking at strange values is with boxplots.
Simply put, boxplots draw a box around the middle 50% of values for 
a given variable, with a bolded horizontal line drawn at the median.
Values that fall far from the bulk of the data points (i.e. outliers)
are denoted by open circles. (If you're curious about the exact
formula for determining what is "far", check out `?hist`.)

In this situation, we are concerned about three things:

1. Since this dataset is about students and the only student
   above the age of 22 is 38 years old, we must wonder whether
   this is an error in the data or just an older student (perhaps
   returning to school after working for several years)
2. There are four values of -1 for the `absences` variable, which
   is either a mistake or an international coding meant to say,
   for example, "this value is missing"
3. There are several extreme values of `abscences` in the positive
   direction, with a maximum value of 75 (which is over 18 times
   the median value of 4)

```{r eval=FALSE}
# View a boxplot of age
boxplot(students3$age)

# View a boxplot of absences
boxplot(students3$absences)
```

### Putting it all together

In this chapter, you will practice everything you've learned from 
the first three chapters in order to clean a messy dataset using R.

#### Get a feel for the data

```{r echo=FALSE}
weather <- read.csv("weather.csv")
weather <- weather[,2:36]
```

```{r}
# Verify that weather is a data.frame
class(weather)

# Check the dimensions
dim(weather)

# View the column names
names(weather)
```

#### Summarize the data

Next up is to look at some summaries of the data. This is where
functions like `str()`, `glimpse()` from `dplyr`, and `summary()` 
come in handy.

```{r}
# View the structure of the data
str(weather)

# Load dplyr package
library(dplyr)

# Look at the structure using dplyr's glimpse()
glimpse(weather)

# View a summary of the data
summary(weather)
```

Now that we have a pretty good feel for how the table is structured, 
we'll take a look at some real observations!

#### Take a closer look

After understanding the structure of the data and looking at some
brief summaries, it often helps to preview the actual data. The 
functions `head()` and `tail()` allow you to view the top and bottom
rows of the data, respectively. Recall you'll be shown 6 rows by
default, but you can alter this behavior with a second argument to 
the function.

```{r}
# View first 6 rows
head(weather)

# View first 15 rows
head(weather, n = 15)

# View the last 6 rows
tail(weather)

# View the last 10 rows
tail(weather, n = 10)
```

#### Column names are values

The `weather` dataset suffers from one of the five most common symptoms
of messy data: column names are values. In particular, the column names
`X1` - `X31` represent days of the month, which should really be values
of a new variable called `day`.

The `tidyr` package provides the `gather()` function for exactly this
scenario. To remind you of how it works, try it with the `df` dataset.

```{r echo=FALSE}
subject <- c("X", "Y", "Z")
age <- c(34, 88, 35)
t1 <- c(0.66753014, 0.3588532, 0.3971684)
t2 <- c(-0.7438284, 0.9038673, 0.7129142)
t3 <- c(0.2333987, -0.5104356, 1.3817088)
df <- data.frame(subject,age,t1,t2,t3)
```
```{r}
# Load the tidyr package
library(tidyr)

df2 <- gather(df, time, val, t1:t3)
df2
```

Notice that `gather()` allows you to select multiple
columns to be gathered by using the `:` operator.

```{r}
# Gather the columns
weather2 <- gather(weather, day, value, X1:X31, na.rm = TRUE)

# View the head
head(weather2)
```

#### Values are variables names

Our data suffer from a second common symptom of messy data: values
are variable names. Specifically, values in the `measure` column
should be variables (i.e. column names) in our dataset. 

The `spread()` function from `tidyr` is designed to help with this.
To remind you of how this function works, we've loaded another small
dataset called `df2`. 

```{r}
spread(df2, time, val)
```

Note how the values of the `time` column now become column names.

```{r}
## The tidyr package is already loaded

# First remove column of row names
weather2 <- weather2[, -1]

# Spread the data
weather3 <- spread(weather2, measure, value)

# View the head
head(weather3)
```

#### Clean up dates

Now that the weather dataset adheres to tidy data principles, the
next step is to prepare it for analysis. We'll start by combining the
year, month, and day columns and recoding the resulting character 
column as a date. We can use a combination of `base` R, `stringr`, and
`lubridate` to accomplish this task.

```{r}
## tidyr and dplyr are already loaded

# Load the stringr and lubridate packages
library(stringr)
library(lubridate)

# Remove X's from day column
weather3$day <- str_replace(weather3$day,"X","")

# Unite the year, month, and day columns
weather4 <- unite(weather3, date, year, month, day, sep = "-")

# Convert date column to proper date format using lubridates's ymd()
weather4$date <- ymd(weather4$date)

# Rearrange columns using dplyr's select()
weather5 <- select(weather4, date, Events, CloudCover:WindDirDegrees)

# View the head of weather5
head(weather5)
```

#### A closer look at column types

It's important for analysis that variables are coded appropriately. This
is not yet the case with our weather data. Recall that function such
as `as.numeric()` and `as.character()` can be used to _coerce_ variables
into different types.

It's important to keep in mind that coercions are not always successfull,
particularly if there's some data in a column that you don't expect.
For example, the following will cause problems:

```
as.numeric(c(4, 6.44, "some string", 222))
```

You'll get a warning message saying that R introduced an `NA` in the
process of coercing to numeric. This is because it doesn't know how
to make a number out of a string (`"some string"`).

```{r}
# View the structure of weather5
str(weather5)

# Examine the first 20 rows of weather5. Are most of the characters numeric?
head(weather5, n = 20)

# See what happens if we try to convert PrecipitationIn to numeric
as.numeric(weather5$PrecipitationIn)
```

#### Column type conversion

As you saw in the last exercise, `"T"` was used to denote a _trace_ 
amount of precipitation in the `PrecipitationIn` column. In order to
coerce this column to numeric, you'll need to deal with this somehow.

```{r}
## The dplyr and stringr packages are already loaded

# Replace T with 0 (T = trace)
weather5$PrecipitationIn <- str_replace(weather5$PrecipitationIn, "T", "0")

# Convert characters to numerics
weather6 <- mutate_each(weather5, funs(as.numeric), CloudCover:WindDirDegrees)

# Look at result
str(weather6)
```

It looks like our data are finally in the correct formats and organized 
in a logical manner! Now that our data are in the right form, 
we can begin the analysis.

#### Find missing values

Before dealing with missing values in the data, it's important to find
them and figure out why they exist in the first place. If your dataset
is too big to look at all at once, like it is here, remember you can
use `sum()` and `is.na()` to quickly size up the situation by
counting the number of `NA` values.

The `summary()` function may also come in handy for identifying
which variables contain the missing values. Finally, the `which()`
function is useful for locating the missing values within a particular
column.

```{r}
# Count missing values
sum(is.na(weather6))

# Find missing values
summary(weather6)

# Find indices of NAs in Max.Gust.SpeedMPH
ind <- which(is.na(weather6$Max.Gust.SpeedMPH))

# Look at the full rows for records missing Max.Gust.SpeedMPH
weather6[ind, ]
```

In this situation it's unclear why these values are missing and there
doesn't appear to be any obvious pattern to their missingness, 
so we'll leave them alone for now.

#### An obvious error

Besides missing values, we want to know if there are values in the data
that are too extreme or bizarre to be plausible. A great way to start
the search for these values is with `summary()`.

Once implausible values are identified, they must be dealt with in an
intelligent and informed way. Sometimes the best way forward is obvious
and other times it may require some research and/or discussions with
the original collectors of the data.

```{r}
# Review distributions for all variables
summary(weather6)

# Find row with Max.Humidity of 1000
ind <- which(weather6$Max.Humidity == 1000)

# Look at the data for that day
weather6[ind, ]

# Change 1000 to 100
weather6$Max.Humidity[ind] <- 100
```

Once you find obvious errors, it's not too hard to fix them if you
know which values they should take.

#### Another obvious error

You've discovered and repaired one obvious error in the data, but it
appears that there's another. Sometimes you get lucky and can infer 
the correct or intended value from the other data. For example, if you
know the minimum and maximum values of a particular metric on a
given day...

```{r}
# Look at summary of Mean.VisibilityMiles
summary(weather6$Mean.VisibilityMiles)

# Get index of row with -1 value
ind <- which(weather6$Mean.VisibilityMiles == -1)

# Look at full row
weather6[ind,]

# Set Mean.VisibilityMiles to the appropriate value
weather6$Mean.VisibilityMiles[ind] <- 10
```

#### Check other extreme values

In addition to dealing with obvious errors in the data, we want
to see if there are other extreme values. In addition to the trusty
`summary()` function, `hist()` is useful for quickly getting a feel for
how different variables are distributed.

```{r}
# Review summary of full data once more
summary(weather6)

# Look at histogram for MeanDew.PointF
hist(weather6$MeanDew.PointF)

# Look at histogram for Min.TemperatureF
hist(weather6$Min.TemperatureF)

# Compare to histogram for Mean.TemperatureF
hist(weather6$Mean.TemperatureF)
```

#### Finishing touches

Before officially calling our weather data clean, we want to put 
a couple of finishing touches on the data. These are a bit more
subjective and may not be necessary for analysis, but they will 
make the data easier for others to interpret, which is generally
a good thing.

There are a number of stylistic conventions in the R language.
Depending on who you ask, these conventions may vary. Because
the period (`.`) has special meaning in certain situations, we
generally recommend using underscores (`_`) to separate words in
variable names. We also prefer all lowercase latters son that no
one has to remember which letters are uppercase or lowercase.

Finally, the `events` column (renamed to be all lowercase in the
first instruction) contains an empty string (`""`) for any day on
which there was no significant weather event such us rain, fog,
a thunderstorm, etc. However, if it's the first time you're seeing
these data, it may not be obvious that this is the case, so it's
best for us to be explicit and replace the empty strings with 
something more meaningful.

```{r}
# Clean up column names
names(weather6) <- tolower(names(weather6))
names(weather6) <- str_replace_all(names(weather6),"\\.","_")
# Replace empty cells in events column
weather6$events[weather6$events == ""] <- "None"
    
# Print the first 6 rows of weather6
head(weather6)
```

