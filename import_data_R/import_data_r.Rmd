---
title: "Import Data with R"
author: "GT"
date: "6 novembre 2016"
output: html_document
---

## Part 1

### Importing data from flat file...

Lots of data comes in the form of flat files: simple tabular text files.
Learn how to import all common formats of flat file data with base R
functions.

#### read.csv

The `utils` package, which is automatically loaded in you R session
on startup, can import CSV files with the `read.csv()` function.

```{r eval=FALSE}
# Import swimming_pools.csv
pools <- read.csv("swimming_pools.csv")

# Print the structure of pools
str(pools)
```

#### stringAsFactors

With `stringAsFactors`, you can tell R whether it should convert
strings in the flat file to factors. 

For all importing functions in the `utils` package, this argument is
`TRUE`, which means that you import strings as factors. This only
makes sense if the strings you import represent categorical variables
in R. If you set `stringAsFactors` to `FALSE`, the data frame columns
corresponding to strings in your text file will be `character`.

```{r eval=FALSE}
# Import swimming_pools.csv correctly: pools
pools <- read.csv("swimming_pools.csv", stringsAsFactors = FALSE)

# Check the structure of pools
str(pools)
```

#### read.delim

Aside from `.csv` files, there are also the `.txt` files which are
basically text files. You can import these functions with `read.delim()`.
By default, it sets the `sep` argument to `"\t"` (fields in a record
are delimited by tabs) and the `header` argument to `TRUE` (the first
row contains the field names).

```{r eval=FALSE}
# Import hotdogs.txt: hotdogs
hotdogs <- read.delim("hotdogs.txt", header = FALSE)

# Summarize hotdogs
summary(hotdogs)
```

#### read.table

If you're dealing with more exotic flat file formats, you'll want to use
`read.table()`. It's the most basic importing function; you can specify
tons of different arguments in this function. Unlike `read.csv()` and
`read.delim()`, the `header` argument defaults to `FALSE` and the `sep`
argument is `""` by default.

```{r eval=FALSE}
# Path to the hotdogs.txt file: path
path <- file.path("data", "hotdogs.txt")

# Import the hotdogs.txt file: hotdogs
hotdogs <- read.table(path, 
                      sep = "\t", 
                      col.names = c("type", "calories", "sodium"))

# Call head() on hotdogs
head(hotdogs)
```

#### Arguments

You can specify the column names of the data frame.

```{r eval=FALSE}
hotdogs <- read.delim("hotdogs.txt", 
                      header = FALSE, 
                      col.names = c("type",
                                    "calories",
                                    "sodium"))

# Selct the hot dog with the leas calories: lily
lily <- hotdogs[which.min(hotdogs$calories),]
```


#### Column classes

Next to column names, you can also specify the column types or column
classes of the resulting data frame. You can do this by setting the 
`colClasses` argument to a vector of strings representing classes:

```
read.delim("my_file.txt",
            colClasses = c("character",
                            "numeric",
                            "logical"))
```

This approach can be useful if you have some columns that should be
factors and others that should be characters. You don't have to bother
with `stringAsFactors` anymore; just state for each column what the 
class should be.

If a column is set to `"NULL"` in the `colClasses` vector, this
column will be skipped and will not be loaded into the data frame.

```{r eval=FALSE}
# Import the data correctly: hotdogs2
hotdogs2 <- read.delim("hotdogs.txt", header = FALSE, 
                       col.names = c("type", "calories", "sodium"),
                       colClasses = c("factor", "NULL", "numeric"))

# Display structure of hotdogs2
str(hotdogs2)
```

### readr & data.table

Next to base R, there are also dedicated packages to easily and efficiently
import flat file data. We'll talk about two such packages: readr and data.table.

#### read_csv

CSV files can be imported with `read_csv()`. It's a wrapper function around
`read_delim()` that handles all the details for you. For example, it will
assume that the first row contains the column names.

```{r eval=FALSE}
# Load the readr package
library(readr)

# Import potatoes.csv with read_csv(): potatoes
potatoes <- read_csv("potatoes.csv")
```

#### read_tsv

Where you use `read_csv()` to easily read in CSV files, you use `read_tsv()`
to easily read in TSV files. TSV is short for tab-separated values.

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import potatoes.txt: potatoes
potatoes <- read_tsv("potatoes.txt", col_names = properties)

# Call head() on potatoes
head(potatoes)
```

#### read_delim

Just as `read.table()` was the main `utils` function, `read_delim()` is the
main `readr` function.

`read_delim()` takes two mandatory arguments:

- `file`: the file that contains the data
- `delim`: the character that separates the values in the data file

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import potatoes.txt using read_delim(): potatoes
potatoes <- read_delim("potatoes.txt", delim = "\t", col_names = properties)
```

#### skip and n_max

Through `skip` and `n_max` you can control which part of your flat file
you're actually importing into R.

- `skip` specifies the number of lines you're ignoring in the flat file
  before actually starting to import data.
- `n_max` specifies the number of lines you're actually import.

Say for example you have a CSV file with 20 lines, and set `skip = 2` and
`n_max = 3`, you're only reading in lines 3, 4 and 5 of the file.

Watch Out: Once you `skip` some lines, you also skip the first line that
can contain column names!

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import 5 observations from potatoes.txt: potatoes_fragment
# Import observations 7, 8, 9, 10 and 11
potatoes_fragment <- read_tsv("potatoes.txt", skip = 6, n_max = 5, 
                              col_names = properties)
```

#### col_types

You can also specify which types the columns in your imported data frame
should have. You can do this with `col_types`. If set to `NULL`, the
default, functions from the `readr` package will try to find the correct
types themeselves. You can manually set the types with a string, where
each character denotes the class of the column:

- `c`haracter
- `d`ouble
- `i`nteger
- `l`ogical
- `_` skips the column as a whole.

```{r eval=FALSE}
# readr is already loaded

# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import all data, but force all columns to be character: potatoes_char
potatoes_char <- read_tsv("potatoes.txt", col_types = "cccccccc", col_names = properties)

# Print out structure of potatoes_char
str(potatoes_char)
```

#### col_types with collectors

Another way of setting the types of the imported columns is using
**collectors**. Collector functions can be passed in a `list()` to
the `col_types` argument of `read_` functions to tell them how to
interpret values in a column.

For a complete list of collector functions, you can take a look at 
the `collector` documentation. For this exercise you will need
two collector functions:

- `col_integer()`: the column should be interpreted as an integer.
- `col_factor(levels, ordered = FALSE`): the column should be 
  interpreted as a factor with `levels`.
  
  
```{r eval=FALSE}
# readr is already loaded

# The collectors you will need to import the data
fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))
int <- col_integer()

# Edit the col_types argument to import the data correctly: hotdogs_factor
hotdogs_factor <- read_tsv("hotdogs.txt",
                           col_names = c("type", "calories", "sodium"),
                           col_types = list(fac, int, int))

# Display the summary of hotdogs_factor
summary(hotdogs_factor)
```

#### fread

You still remember how to use `read.table()`, right? Well, `fread()`
is a function that does the same job withg very similar argument.
It is extremely easy to use and blazingly fast! Often, simply specifying
the path to the file is enough to successfully import your data.

```{r eval=FALSE}
# load the data.table package
library(data.table)

# Import potatoes.csv with fread(): potatoes
potatoes <- fread("potatoes.csv")

# Print out potatoes
potatoes
```

#### fread: more advanced use

Now that you know the basic about `fread()`, you should know about
two arguments of the function `drop` and `select` to drop or select
variables of interest.

Suppose you have a dataset that contains 5 variables and you want 
to keep the first and fifth varible, named "a" and "e". The following
options will all do the trick:

```
fread("path/to/file.txt", drop = 2:4)
fread("path/to/file.txt", select = c(1,5))
fread("path/to/file.txt", drop = c("b", "c", "d"))
fread("path/to/file.txt", select = c("a", "e"))
```
```{r eval=FALSE}
# fread is already loaded

# Import columns 6 and 8 of potatoes.csv: potatoes
potatoes <- fread("potatoes.csv", select = c(6, 8))

# Plot texture (x) and moistness (y) of potatoes
plot(potatoes$texture, potatoes$moistness)
```

#### Dedicated classes

You might have noticed that the `fread()` function produces data
frames that look slightly different when you print them out.
That's because another class named `data.table` is assigned to
the resulting data frames. The printout of such `data.table` objects
is different. 

The class of the result of `fread()` is both `data.table` and
`data.frame`. `read_tsv()` creates an object with three classes:
`tbl_df`, `tbl` and `data.frame`.

What's the benefit of these additional classes? Well, it allows
for a different treatment of printouts, for example.

### Importing Excel data

Excel is very widely used data analysis tool. If you prefer to
do your analyses in R, though, you'll need an understanding of
importing CSV data into R. This chapter will show you how to use
readxl and gdata to do so.

#### List the sheets of an Excel file

Before you can start importing from Excel, you should find
out which sheets are available in the workbook. You can
use the `excel_sheets()` function for this.

```{r eval=FALSE}
# Load the readxl package
library(readxl)

# Print out the names of both spreadsheets
excel_sheets("urbanpop.xlsx")
```

`excel_sheets()` is a simple character vector; you haven't
imported anything yet.

#### Import an Excel sheet

Now that you know the names of the sheets in the Excel file
you want to import, it is time to import those  sheets into R.
You can do this with the `read_excel()` function. Have a look
at this recipe:

```
data <- read_excel("data.xlsx", sheet = "my_sheet")
```

This call simply imports the sheet with the name `"my_sheet"` 
from the `"data_xlsx"` file. You can also pass a number to
the `sheet` argument; this will cause `read_excel()` to
import the sheet with the given sheet number. `sheet = 1`
will import the first sheet, `sheet = 1` will import the
second sheet, and so on.

```{r eval=FALSE}
# The readxl package is already loaded

# Read the sheets, one by one
pop_1 <- read_excel("urbanpop.xlsx", sheet = 1)
pop_2 <- read_excel("urbanpop.xlsx", sheet = 2)
pop_3 <- read_excel("urbanpop.xlsx", sheet = 3)

# Put pop_1, pop_2 and pop_3 in a list: pop_list
pop_list <- list(pop_1, pop_2, pop_3)

# Display the structure of pop_list
str(pop_list)
```

#### Reading a workbook

In the previous exercise you generated a list of three Excel
sheets that you imported. However, loadin in every sheet
manually and then merging them in a list can be quite tedious.
Luckily, you can automate this with `lapply()`.

How a look at the example code below:

```
my_workbook <- lapply(excel_sheets("data.xlsx"),
                      read_excel,
                      path = "data.xlsx")
```

The `read_excel()` function is called multiple times on the
`"data.xlsx"` file and each sheet is loaded in one afterr the
other. The result is a list of data frames, each data frame 
representing one of the sheets in `data.xlsx`.

```{r eval=FALSE}
# The readxl package is already loaded

# Read all Excel sheets with lapply(): pop_list
pop_list <- lapply(excel_sheets("urbanpop.xlsx"),
                    read_excel,
                    path = "urbanpop.xlsx")

# Display the structure of pop_list
str(pop_list)
```

#### The col_names argument

Apart from `path` and `sheet`, there are several othe arguements you
can specify in `read_excel()`. One of these arguments is called
`col_names`.

By default it is `TRUE`, denoting whether the first row in the Excel
sheets contains the column names. If this is not the case, you
can set `col_names` to `FALSE`. In this case, R will choose column
names for you. You can also choose to set `col_names` to a 
character vector with names for each column. It works exactly
the same as in the `readr` package.

```{r eval=FALSE}
# The readxl package is already loaded

# Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a
pop_a <- read_excel("urbanpop_nonames.xlsx", sheet = 1, col_names = FALSE)

# Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b
cols <- c("country", paste0("year_", 1960:1966))
pop_b <- read_excel("urbanpop_nonames.xlsx", sheet = 1, col_names = cols)

# Print the summary of pop_a
summary(pop_a)

# Print the summary of pop_b
summary(pop_b)
```

It's really crucial to correctly tell R whether your Excel data
contains column names. If you don't, the head of the data frame
you end up with will contain incorrect information...

#### The skip argument

Another argument that can be very useful when reading in Excel files
that are less tidy, is `skip`. With `skip`, you can tell R to ignore
a specified number of rows inside the Excel sheets you're trying to
pull data from. Have a look at this example:

```
read_excel("data.xlsx, skip = 15")
```

In this case, the first 15 rows in the first sheet of `"data.xlsx"` 
are ignored.

If the first row of this sheet contained the column names, this
information will also be ignored by `readxl`. Make sure to set
`col_names` to `FALSE` or manually specify column names in this case!

```{r eval=FALSE}
# The readxl package is already loaded

# Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel
urbanpop_sel <- read_excel("urbanpop.xlsx", sheet = 2, skip = 21, col_names = FALSE)

# Print out the first observation from urbanpop_sel
urbanpop_sel[1,]
```

#### Import a local file

In this part of the chapter you'll learn how to import `.xls` files
using the `gdata` package. Similar to `readxl` package, you can import
sigle Excel sheets to start your analysis in R.

```{r eval=FALSE}
# Load the gdata package
library(gdata)

# Import the second sheet of urbanpop.xls: urban_pop
urban_pop <- read.xls("urbanpop.xls", sheet = "1967-1974")

# Print the first 11 observations using head()
head(urban_pop, n = 11)
```

#### read.xls() wraps around read.table()

Remember how `read.xls()` actually works ? It basically comes down
to two steps: converting the Excel file to a `.csv` file using a Perl
script, and then reading that `.csv` file with the `read.csv()` function
that is loaded by default in R, through the `utils` package. 

This means that all the options that you can specify in `read.csv()`,
can also specified in `read.xls()`.

```{r eval=FALSE}
# The gdata package is alreaded loaded

# Column names for urban_pop
columns <- c("country", paste0("year_", 1967:1974))

# Finish the read.xls call
urban_pop <- read.xls("urbanpop.xls", sheet = 2,
                      skip = 50, header = FALSE, stringsAsFactors = FALSE,
                      col.names = columns)

# Print first 10 observation of urban_pop
head(urban_pop, n = 10)
```

#### Work that Excel data!

Now that you can read in Excel data, let's try to clean and merge it. 
You already used the `cbind()` function. Let's take it one step
further now.

```{r eval=FALSE}
# Add code to import data from all three sheets in urbanpop.xls
path <- "urbanpop.xls"
urban_sheet1 <- read.xls(path, sheet = 1, stringsAsFactors = FALSE)
urban_sheet2 <- read.xls(path, sheet = 2, stringsAsFactors = FALSE)
urban_sheet3 <- read.xls(path, sheet = 3, stringsAsFactors = FALSE)

# Extend the cbind() call to include urban_sheet3: urban
# Make sure the first column for urban_sheet2 and urban_sheet3 are
# removed to avoid duplicate columns.
urban <- cbind(urban_sheet1, urban_sheet2[-1], urban_sheet3[-1])

# Remove all rows with NAs from urban: urban_clean
urban_clean <- na.omit(urban)

# Print out a summary of urban_clean
summary(urban_clean)
```

### Reproducible Excel work with XLConnect

Next to importing data from Excel, you can take things one step further with
XLConnect. Learn all about it and bridge the gap between R and Excel!

#### Connect to a workbook

When working with `XLConnect`, the first step will be to load a
workbook in your R session with `loadWorkbook()`; this function
will build a "bridge" between your Excel file and your R session.

```{r eval=FALSE}
# Load the XLConnect package
library(XLConnect)

# Build connection to urbanpop.xlsx: my_book
my_book <- loadWorkbook("urbanpop.xlsx")

# Print out the class of my_book
class(my_book)
```

#### List and read Excel sheets

Just as `readxl` and `gdata`, you can use `XLConnect` to import
data from Excel file into R.

To list the sheets in an Excel file, use `getSheets()`. To actually
import data from a sheet, you can use `readWorksheet()`. Both
function require an XLConnect workbook object as the first
argument.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# List the sheets in my_book
getSheets(my_book)

# Import the second sheet in my_book
readWorksheet(my_book, sheet = 2)
```

#### Customize readWorksheet

To get a clear overview about `urbanpop.xlsx` without having
to open up the Excel file, you can execute the following code:

```
my_book <- loadWorkbook("urbanpop.xlsx")
sheets <- getSheets(my_book)
all <- lapply(sheets, readWorksheet, object = my_book)
str(all)
```

Suppose we're only interested in urban population data of the
years 1968, 1969 and 1970. The data for these years is in the
columns 3, 4 and 5 of the second sheet. Only selecting these
columns will leave us in the dark about the actual countries
the figures belong to.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Import columns 3, 4, and 5 from second sheet in my_book: urbanpop_sel
urbanpop_sel <- readWorksheet(my_book, sheet = 2, startCol = 3, endCol = 5)

# Import first column from second sheet in my_book: countries
countries <- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1)

# cbind() urbanpop_sel and countries together: selection
selection <- cbind(countries, urbanpop_sel)
```

#### Add worksheet

Where `readxl` and `gdata` were only able to import Excel data,
`XLConnect`'s approach of providing an actual interface to an
Excel file makes it able to edit your Excel files from inside
R. In this step, you'll create a new sheet.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Use getSheets() on my_book
getSheets(my_book)
```

#### Populate worksheet

The first step of creating a sheet is done; let's populate it
with some data now! `summ`, a data frame with some summary
statistics on the two Excel sheets is already coded so you can
take it from there.

```{r eval=FALSE}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Create data frame: summ
sheets <- getSheets(my_book)[1:3]
dims <- sapply(sheets, function(x) dim(readWorksheet(my_book, sheet = x)), USE.NAMES = FALSE)
summ <- data.frame(sheets = sheets,
                   nrows = dims[1, ],
                   ncols = dims[2, ])

# Add data in summ to "data_summary" sheet
writeWorksheet(my_book, summ, sheet = "data_summary")

# Save workbook as summary.xlsx
saveWorkbook(my_book, file = "summary.xlsx")
```

#### Renaming sheets

Come to think of it, `"data_summary"` is not an ideal name. As the
summary of these excel sheets is always data-related, you
simply want to name the sheet `"summary"`.

```{r eval=FALSE}
# my_book is available

# Rename "data_summary" sheet to "summary"
renameSheet(my_book, "data_summary", "summary")

# Print out sheets of my_book
getSheets(my_book)

# Save workbook to "renamed.xlsx"
saveWorkbook(my_book, file = "renamed.xlsx")
```

#### Removing sheets

After presenting the new Excel sheet to your peers, it appears
not everybody is a big fan. Why summarize sheets and store
the info in Excel if all the information is implicitly
available? To hell with it, just remove the entire fourth
sheet!

```{r eval=FALSE}
# Load the XLConnect package
library(XLConnect)

# Build connection to renamed.xlsx: my_book
my_book <- loadWorkbook("renamed.xlsx")

# Remove the fourth sheet
removeSheet(my_book, sheet = "summary")

# Save workbook to "clean.xlsx"
saveWorkbook(my_book, file = "clean.xlsx")
```

## Part 2

### Importing data from databases

Many companies store their information in relational databases.
The R community has also developed R packages to get data from
these architectures. You'll learn how to connect to a database and
how to retrieve data from it.

#### Establish a connection

The first step to import data from a SQL database is creating a
connection to it. You need different packages depending on the
database you want to connect to. All of these packages do this
in a uniform way, as specified in the `DBI` package.

`dbConnect()` creates a connection between your R session and
a SQL database. The first argument has to be a `DBIdriver` object,
that specifies how connections are made and how data is mapped
between R and the database. Specifically for MySQL databases, you
can build such a driver with `RMySQL::MySQL()`.

If the MySQL database is a remote database hosted on a server,
you'll also have to specify the followinng argumements in
`dbConnect()`: `dbname`, `host`, `port`, `user` and `password`.

```{r eval=FALSE}
# Load the DBI package
library(DBI)

# Edit dbConnect() call
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "tweater", 
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                 port = 3306,
                 user = "student",
                 password = "datacamp")
```

#### Inspect the connection

Now that you've successfully created the database connection,
let's have a closer look at it.

```
class(con)
[1] "MySQLConnection"
attr(,"package")
[1] "RMySQL"
```

con is an `MySQLConnection` object.

#### List the database tables

After you've successfully connected to a remote MySQL database,
the next step is to see what tables the database contains. You can
do this with the `dbListTables()` function. As you might remember
from the video, this function requires the connection object as
an input, and outputs a character vector with the table names.

```{r eval=FALSE}
# Build a vector of table names: tables
tables <- dbListTables(con)

# Display structure of tables
str(tables)
```

`dbListTables()` can be very useful to get a first idea about the
contents of your database. Can you guess what kind of information
this database contains?

#### Import users

As you might have guessed by now, the database contains data on a
more tasty version of Twitter, namely Tweater. Users can post 
tweats with short recipes for delicious snacks. People can comment
on these tweats. There are three tables: **users**, **tweats** and
**comments** that have relations among them. Which ones, you ask?

Let's start by importing the data on the users into your R session.
You do this with the `dbReadTable()` function. Simply pass it the
connection object (`con`), followed by the name of the table you
want to import. The resulting object is a standard R data frame.

```{r eval=FALSE}
# Import the users table from tweater: users
users <- dbReadTable(con, "users")

# Print users
users
```

#### Import all tables

Next to the `users`, we're also interested in the `tweats` and
`comments` tables. However, separate `dbReadTable()` call for each
and every one of the tables in your database would mean a lot of
code duplication. Remember about the `lapply()` function? You can
use it again here! 

```{r eval=FALSE}
# Get table names
table_names <- dbListTables(con)

# Import all tables
tables <- lapply(table_names, dbReadTable, conn = con)

# Print out tables
tables
```

Now that you have an R version of all data that is contained in
the database, you can dive a little deeper into the relations
between the different data frames.

#### How do the tables relate ?

If you have a closer look at these tables, you'll see that the
`tweats` table, for example, contains a column `user_id`. The ids 
in the column refer to the users that have posted the tweat.
Similarly, the `comments` contain both a `user_id` and a `tweat_id`
column. It specifies which user posted a comment on which tweat.

With this new knowledge, can you tell **who** posted the **tweat** on
which somebody **commented** "awesome!thanks!" (comment 1012)?

```
> tables
$comments
     id tweat_id user_id            message
1  1022       87       7              nice!
2  1000       77       7             great!
3  1011       49       5            love it
4  1012       87       1   awesome! thanks!
5  1010       88       6              yuck!
6  1026       77       4      not my thing!
7  1004       49       1  this is fabulous!
8  1030       75       6           so easy!
9  1025       88       2             oh yes
10 1007       49       3           serious?
11 1020       77       1 couldn't be better
12 1014       77       1       saved my day

$tweats
  id user_id
1 75       3
2 88       4
3 77       6
4 87       5
5 49       1
6 24       7
                                                                 post
1                                       break egg. bake egg. eat egg.
2                           wash strawberries. add ice. blend. enjoy.
3                       2 slices of bread. add cheese. grill. heaven.
4               open and crush avocado. add shrimps. perfect starter.
5 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins.
6                              just eat an apple. simply and healthy.
        date
1 2015-09-05
2 2015-09-14
3 2015-09-21
4 2015-09-22
5 2015-09-22
6 2015-09-24

$users
  id      name     login
1  1 elisabeth  elismith
2  2      mike     mikey
3  3      thea   teatime
4  4    thomas tomatotom
5  5    oliver olivander
6  6      kate  katebenn
7  7    anjali    lianja
```

Answer: The user with user_id 5, so Oliver.

### Importing data from databases

Importing an entire table from a database while you might only 
need a tiny bit of information seems like a lot of unncessary work. 
In this chapter, you'll learn about SQL queries, which will help 
you make things more efficient by performing some computations 
on the database side.

#### Query tweater

If your life as a data scientist, you'll often be working with 
huge databases that contain tables with millions of rows. If
want to do some analyses on this data, it's possible that you 
only need a fraction of this data. In this case, it's a good idea
to send SQL queries to your database, and only import the data
you actually need into R.

`dbGetQuery()` is what you need. As usual, you first pass the 
connection object to it. The second argument is an SQL query
in the form of a character string. This example selects the `age`
variable from the `people` dataset where `gender` equals `"male"`:

```
dbGetQuery(con, "SELECT age FROM people WHERE gender = 'male'")
```

```{r eval=FALSE}
# Import tweat_id column of comments where user_id is 1: elisabeth
elisabeth <- dbGetQuery(con, "SELECT tweat_id FROM comments WHERE user_id = 1")

# Print elisabeth
elisabeth
```

Apart from checking equality, you can also check for _less than_ and
_greater than_ relationships, with `<` and `>`, just like R.

```{r eval=FALSE}
# Import post column of tweats where date is higher than '2015-09-21': latest
latest <- dbGetQuery(con, "SELECT post FROM tweats WHERE date > '2015-09-21'")

# Print latest
latest
```

Suppose that you have a `people` table, with a bunch of information.
This time, you want to find out the `age` and `country` of married
males. Provided that there is  a `married` column that's 1 when the 
person in question is married, the following query would work.

```
SELECT age, country
  FROM people
    WHERE gender = "male" AND married = 1
```

```{r eval=FALSE}
# Create data frame specific
specific <- dbGetQuery(con, "SELECT message FROM comments 
                            WHERE tweat_id = 77 AND user_id > 4")

# Print specific
specific
```

There are also dedicated SQL functions that you can use in the
`WHERE` clause of an SQL query. For example, `CHAR_LENGTH()` returns
the number of characters in a string.

```{r eval=FALSE}
# Create data frame short
short <- dbGetQuery(con, "SELECT id, name FROM users 
                          WHERE CHAR_LENGTH(name) < 5")

# Print short
short
```

#### Join the query madness!

Of course, SQL does not stop with the three keywords `SELECT`,
`FROM` and `WHERE`. Another very often used keyword is `JOIN`,
and more specifically `INNER JOIN`. Take this call for example:

```
SELECT name, post
  FROM users INNER JOIN tweats on users.id = user_id
    WHERE date > "2015-09-19"
```
Here, the `users` table is joined with the `tweats`. This is
possible because the `id` column in the `users` table corresponds
to the `user_id` column in the `tweats`. Also notice how `name`,
from the `users` table, and `post` and `date`, from the `tweats`
table, can be referenced without problems.

#### Send - Fetch - Clear

You've used `dbGetQuery()` multiple times now. This is a virtual
function from the `DBI package`, but is actually implemented by the
`RMySQL` package. Behind the scenes, the following steps are
performed:

- Sending the specified query with `dbSendQuery();
- Fetching the result of executing the query on the database
  with `dbFetch();
- Clearing the result with `dbClearResult()`.

Let's not use `dbGetQuery()` this time and implement the steps
above. This is tedious to write, but it gives you the ability
te fetch the query's result in chunks rather than all at once.
You can do this by specifying the `n` argument inside `dbFetch()`.

```{r eval=FALSE}
# Send query to the database
res <- dbSendQuery(con, "SELECT * FROM comments WHERE user_id > 4")

# Use dbFetch() twice
dbFetch(res, n = 2)
dbFetch(res)

# Clear res
dbClearResult(res)
```

In our toy example, chopping up the fetches doesn't make a lot of sense,
but make sure to remember this technique when you're struggling with
huge databases!

#### Be polite and ...

Every time you connect to a database using `dbConnect()`, you're
creating a new connection to the database you're referencing.
`RMySQL` automatically specifies a maximum of open connections and
closes some of the connections for you, but still: it's always polite
to manually disconnect from the database afterwards. You do this
with the `dbDisconnect()` function.

```{r eval=FALSE}
# Create the data frame  long_tweats
long_tweats <- dbGetQuery(con, "SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40")

# Print long_tweats
print(long_tweats)

# Disconnect from the database
dbDisconnect(con)
```

### Importing data from the web

More and more of the information that data scientists are using 
resides on the web. Importing this data into R requires an 
understanding of the protocols used on the web.
In this chapter, you'll get a crash course in HTTP and learn 
to perform your own HTTP requests from inside R.

#### Import flat files from the web

In the video, you saw that the `utils` functions to import flat
file data, such as `read.csv()` and `read.delim()`, are capable
of automatically importing from URLs that point to flat files
on the web.

You must be wondering whether Wickham's alternative package, `readr`,
is equally potent.

```{r eval=FALSE}
# Load the readr package
library(readr)

# Import the csv file: pools
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
pools <- read_csv(url_csv)

# Import the txt file: potatoes
url_delim <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt"
potatoes <- read_tsv(url_delim)

# Print pools and potatoes
pools
potatoes
```

Great! It seems to work without any additional problems!

#### Secure Importing

In the previous exercises, you have been working with URLs that
all start with `http://`. There is, however, a safer alternative
to HTTP, namely HTTPS, which stands for HypterText Transfer Protocol
Secure. Just remember this: HTTPS is relatively safe, HTTP is not.

Luckily for us, you can use the standard importing functions with
`https://` connections since R version 3.2.2.

```{r eval=FALSE}
# https URL to the swimming_pools csv file.
url_csv <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"

# Import the file using read.csv(): pools1
pools1 <- read.csv(url_csv)
```

#### Import Excel files from the web

When you learned about `gdata`, it was already mentioned that
`gdata` can handle `.xls` files that are on the internet. 
`readxl` can't, at least not yet. The URL which you'll be working
is already available in the sample code. You will import it
once using `gdata` and once with the `readxl` package via a 
workaround.

```{r eval=FALSE}
# Load the readxl and gdata package
library(readxl)
library(gdata)

# Specification of url: url_xls
url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# Import the .xls file with gdata: excel_gdata
excel_gdata <- read.xls(url_xls)

# Download file behind URL, name it local_latitude.xls
local_latitude <- download.file(url_xls, "local_latitude.xls")

# Import the local .xls file with readxl: excel_readxl
excel_readxl <- read_excel("local_latitude.xls")
```

#### Downloading any file, secure or not

In the previous exercise you've seen how you can read excel files
on the web using the `read_excel` package by first downloading the 
file with the `download.file()` function.

There's more: with `download.file()` you can download any kind of 
file from the web, using HTTP and HTTPS: images, executable files,
but also `.RData` files. An `RData` file is very efficient format
to store R data.

You can load data from an `RData` file using the `load()` function,
but this function does not accept a URL string as an argument.

```{r eval=FALSE}
# https URL to the wine RData file.
url_rdata <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData"

# Download the wine file to your working directory
download.file(url_rdata, "wine_local.RData")

# Load the wine data into your workspace using load()
load("wine_local.RData")

# Print out the summary of the wine data
summary(wine)
```

Another way to load remote `RData` files is to use `url()` function
inside `load()`. However, this will not save the `RData` file
to a local file.

#### HTTP?httr!

Downloading a file from the Internet means sending a GET request
and receiving  the file you asked for. Internally, all the 
previously discussed functions use a GET request to download files.

`httr` provides a convenient function, `GET()` to execute this
GET request. The result is a `response` object, that provides easy
access to the status code, content-type and, of course, the actual
content.

You can extract the content from the request using `content()` 
function. At the time of writing, there are three ways to retrieve
this content: as a rawo object, as a character vector, or an
R object, such a list. If you don't tell `content()` how to retrieve
the content through the `as` argument, it'll try its best to figure
out which type is most appropriate based on the content-type.

```{r eval=FALSE}
# Load the httr package
library(httr)

# Get the url, save response to resp
url <- "http://www.example.com/"
resp <- GET(url)

# Print resp
resp

# Get the raw content of resp: raw_content
raw_content <- content(resp, as = "raw")

# Print the head of raw_content
head(raw_content)
```

The raw content of the response doesn't make a lot of sense, does it?
Luckily, the `content()` function by default, if you don't specify
the `as` argument, figures out what type of data you're dealing 
with and parses it for you.

Web content does not limit itself to HTML pages and files stored
on remote servers such as DataCamp's Amazon S3 instances. There
are may other data formats out there. A very common one is JSON.
This format is very often used by so-called Web APIs, interfaces
to web servers with which you as client can communicate to get or
store information in more complicated ways.

```{r eval=FALSE}
# httr is already loaded

# Get the url
url <- "https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json"
resp <- GET(url)

# Print resp
resp

# Print content of resp as text
content(resp, as="text")

# Print content of resp
content(resp)
```

The fact that `httr` converts the JSON response body automatically
to an R list is very convenient.

### Importing data from the web

Importing data from the web is one thing; actually being able 
to extract useful information is another. Learn more about the JSON
format to get one step closer to web domination.

#### From JSON to R

In the simplest setting, `fromJSON()` can convert character strings
that represent JSON data into a nicely structured R list.

```{r eval=FALSE}
# Load the jsonlite package
library(jsonlite)

# wine_json is a JSON
wine_json <- '{"name":"Chateau Migraine", "year":1997, "alcohol_pct":12.4, "color":"red", "awarded":false}'

# Convert wine_json into a list: wine
wine <- fromJSON(wine_json)

# Print structure of wine
str(wine)
```

#### Quandl API

`fromJSON()` also works if you pass a URL as a character string or the
path to a local file that contains JSON data. Let's try this out on
the Quandl API, where you can fetch all sorts of financial and 
economical data.

```{r eval=FALSE}
# jsonlite is preloaded

# Definition of quandl_url
quandl_url <- "http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz"

# Import Quandl data: quandl_data
quandl_data <- fromJSON(quandl_url)

# Print structure of quandl_data
str(quandl_data)
```
You successfully imported JSON data directly from the web. 
If you have a close look at the structure of `quandl_data`, 
you'll see that the data element is a matrix.

#### OMDb API

You saw how easy it is to interact with an API once you know how to 
formulate requests. You also saw how to fetch all information on
Rain Man from OMDb. Simply performa `GET()` call, and next ask for
the contents with the `content()` function. This `content()` function,
which is part of the `httr` package, uses `jsonlite` behind the scenes
to import the JSON data into R.

However, by now you also know that `jsonlite` can handle URLs itself
Simply passing the request URL to `fromJSON()` will get your data
into R. 

```{r eval=FALSE}
# The package jsonlite is already loaded

# Definition of the URLs
url_sw4 <- "http://www.omdbapi.com/?i=tt0076759&r=json"
url_sw3 <- "http://www.omdbapi.com/?i=tt0121766&r=json"

# Import two URLs with fromJSON(): sw4 and sw3
sw4 <- fromJSON(url_sw4)
sw3 <- fromJSON(url_sw3)

# Print out the Title element of both lists
sw4$Title
sw3$Title


# Is the release year of sw4 later than sw3?
sw4$Year > sw3$Year
```

#### JSON practice

JSON is built on two structures: objects and arrays.

```{r}
# jsonlite load
library(jsonlite)
# Challenge 1
json1 <- '[1, 2, 3, 4, 5, 6]'
fromJSON(json1)

# Challenge 2
json2 <- '{"a": [1, 2, 3], "b": [4, 5, 6]}'
fromJSON(json2)

# jsonlite is already loaded

# Challenge 1
json1 <- '[[1, 2], [3, 4]]'
fromJSON(json1)

# Challenge 2
json2 <- '[{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a": 5, "b": 6}]'
fromJSON(json2)
```

#### toJSON()

Apart from converting JSON to R with `fromJSON()`, you can also use
`toJSON()` to convert R data to a JSON format. In its most basic
use, you simply pass this function an R object to convert to a JSON.
The result is an R object of the class `json`, which is basically
a character string representing that JSON.

```{r eval=FALSE}
# jsonlite is already loaded

# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"

# Import the .csv file located at url_csv
water <- read.csv(url_csv, stringsAsFactors = FALSE)

# Convert the data file according to the requirements
water_json <- toJSON(water)
   
# Print out water_json
water_json
```

As you can see, the JSON you printed out isn't easy to read. 
In the next exercise, you will print out some more JSONs, and
explore ways to prettify or minify the output.

#### Minify and prettify

JSONs can come in different formats. Take these two JSONs, that
are in fact exactly the same: the first one is in a minified format,
the second one is in a pretty format with indentation, whitespace
and new lines:

```
# Mini
{"a":1,"b":2,"c":{"x":5,"y":6}}

# Pretty
{
  "a": 1,
  "b": 2,
  "c": {
    "x": 5,
    "y": 6
  }
}
```

Unless you're a computer, you surely prefer the second version.
However, the standard form that `toJSON()` returns, is the minified
version, as it is more concise. You can adapt this behavior by
setting the `pretty` argument inside `toJSON()` to `TRUE`. If
you already have a JSON string, you can use `prettify()` or
`minify()` to make the JSON pretty or as concise as possible.

```{r eval=FALSE}
# jsonlite is already loaded

# Convert mtcars to a pretty JSON: pretty_json
pretty_json <- toJSON(mtcars, pretty = TRUE)

# Print pretty_json
pretty_json

# Minify pretty_json: mini_json
mini_json <- minify(pretty_json)

# Print mini_json
mini_json
```

### Importing data from statistical software

Next to R, there are also other commonly used statistical 
software packages: SAS, STATA and SPSS. Each of them has their
own file format. Learn how to use the haven and foreign packages 
to get them into R with remarkable ease!

#### Import SAS data with haven

`haven` is an extremely easy-to-use package to import data from
threes software packages: SAS, STATA and SPSS. Depending on the
software, you use different functions:

- SAS: `read_sas()`
- STATA: `read_dta()` (or `read_stata()`, which are identical)
- SPSS: `read_sav()` or `read_por()`, depending of the file type.

All these functions take one key argument: the path to your local 
file. In fact, you can even pass a URL; `haven` will then automatically
download the file for you before importing.

```{r}
# Load the haven package
library(haven)

# Definition of the URL
url_sales <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/sales.sas7bdat"

# Import sales.sas7bdat: sales
sales <- read_sas(url_sales)

# Display the structure of sales
str(sales)
```

#### Import STATA data with haven

Next up are STATA data files; you can use `read_dta()` for these.

When inspecting the result of the `read_dta` call, you will notice
that one column will be imported as a `labelled` vector, an R 
equivalent for the common data structure in other statistical
environments. In order to effectively continue working on the data
in R, it's best to change this data into standard R class.
To convert a variable of the class `labelled` to a factor, 
you'll need `haven`'s `as_factor()` function.


```{r}
# haven is already loaded

# Import the data from the URL: sugar
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")

# Structure of sugar
str(sugar)

# Convert values in Date column to dates
sugar$Date <- as.Date(as_factor(sugar$Date))

# Structure of sugar again
str(sugar)
```

#### What does the graphic tell?

A plot can be very useful to explore the relationship between two
variables. If you pass the `plot()` function two arguments,
the first one will be on the x-axis, and the second one will be on
the y-axis.

After you've imported the data frame, you should plot two of
its varibles, `Import` against `Weight_I`, and describe their
relationship!

```{r}
plot(sugar$Import, sugar$Weight_I)
```

You can spot an increasing trend among the data points. This of
course makes sense: the more sugar is traded, the higher the weight
that's traded.

#### Import SPSS data with haven

The `haven` package can also import data files from SPSS. Again,
importing the data is pretty straighforward. Depending on the 
SPSS data file you're working with, you'll need either `read_sav()`
for `.sav` files or `read_por()` for `.por` files.

```{r eval=FALSE}
# haven is already loaded

# Import person.sav: traits
traits <- read_sav("person.sav")

# Summarize traits
summary(traits)

# Print out a subset
subset(traits,Extroversion > 40 & Agreeableness > 40)
```

#### Factorize, round two

In the last exercise you learned how to import a data file using
the command `read_sav()`. With SPSS data files, it can also happen
that some of the variables you import have the `labelled` class. 
This is done to keep all the labelling information that was 
originally present in the `.sav` and `.por` files. It's advised
to coerce (or change) these variables to factors or other
standard R classes.

```{r eval=FALSE}
# haven is already loaded

# Import SPSS data from the URL: work
work <- read_sav("employee.sav")

# Display summary of work$GENDER
summary(work$GENDER)


# Convert work$GENDER to a factor
work$GENDER <- as_factor(work$GENDER)


# Display summary of work$GENDER again
summary(work$GENDER)
```

#### Import STATA data with foreign

The `foreign` package offers a simple function to import and
read _STATA_ data: `read.dta()`.

```{r eval=FALSE}
# Load the foreign package
library(foreign)

# Import florida.dta and name the resulting data frame florida
florida <- read.dta("florida.dta")

# Check tail() of florida
tail(florida)
```

Data can be very diverse, going from character vectors to categorical
variables, dates and more. It's in these case that the additional
arguments of `read.dta()` will come in handy.

The arguments you will use most often are `convert.dates`, 
`convert.factors`, `missing.type` and `convert.underscore`. Their
meaining is pretty straightforward. It's all about correctly 
converting STATA data to standard R data structures.

```{r}
library(foreign)

# Specify the file path using file.path(): path
path <- file.path("worldbank", "edequality.dta")
url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/edequality.dta"

# Create and print structure of edu_equal_1
edu_equal_1 <- read.dta(url)
str(edu_equal_1)
```

For this data, the first version (where you simply specified the 
file path) will be most useful to work with.

#### Do you know your data?

The previous exercise dealt about socio-economic indicators and
access to education of different individuals.

For example, you can ask yourself how many observations have an `age`
higher than 40 adn are `literate`.

You see that `age` is an integer, and `literate` is a factor, with
levels "yes" and "no". The following expression thus answers the
question

```{r}
nrow(subset(edu_equal_1, age > 40 & literate == "yes"))
```

How many observations from Bulgaria have an income above 1000 ?

```{r}
nrow(subset(edu_equal_1, ethnicity_head == "Bulgaria" & income > 1000))
```

The data frame contains 8997 individuals with a Bulgarian 
ethnicity whose income is above 1000.

#### Import SPSS data with foreign

All great things come in pairs. Where `foreign` provided `read.dta()`
to read Stata data, there's also `read.spss()` to read SPSS data files.
To get a data frame, make sure to set `to.data.frame = TRUE` inside
`read.spss()`.

```{r}
# foreign is already loaded

url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/international.sav"

# Import international.sav as a data frame: demo
demo <- read.spss(url, to.data.frame = TRUE)

# Create boxplot of gdp variable of demo
boxplot(demo$gdp)
```

#### Excursion: Correlation

If you're familiar with statistic, you'll have heard about
Pearson'Correlation. It is a measurement to evaluate the linear
dependency between two variables, say $X$ and $Y$. It can range
from -1 to 1; if it's close to 1 it means that there is a strong
positive association between the variables. If $X$ is high, also
$Y$ tends to be high. If it's close to -1, there is a strong
negative association: if $X$ is high, $Y$ tends to be low.
When the Pearson correlation between two variables is 0, these
variables are possibly independent: there is no association between
$X$ and $Y$.

You can calculate the correlation between two vectors with the
`cor()` function. Take this code for example, that computes the
correlation between the columns `height` and `width` of a 
fictional data frame `size`:

```
cor(size$height, size$width)
```

What is the correlation coefficient for the two numerical variables
`gdp` and `f_illit` (female illiteracy rate)?

```{r}
cor(demo$gdp, demo$f_illit)
```

That indicates a negative association among GDP and female illiteracy.

#### Import SPSS data with foreign

In the previous exercise, you used the `to.data.frame` argument
inside `read.spss()`. There are many other ways in which to 
customize the way your SPSS data is imported.

In this exercise you will experiment with another argument, 
`use.value.labels`. It specifies whether variables with value
labels should be converted into R factors with levels that are
named accordingly. The argument is `TRUE` by default which means that
so called labelled variable inside SPSS are converted to factor
inside R.

```{r}
# foreign is already loaded

# Import international.sav as demo_1
demo_1 <- read.spss(url, to.data.frame = TRUE)

# Print out the head of demo_1
head(demo_1)

# Import international.sav as demo_2
demo_2 <- read.spss(url, to.data.frame = TRUE, 
use.value.labels = FALSE)

# Print out the head of demo_2
head(demo_2)
```

