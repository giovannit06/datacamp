---
title: "Titanic"
author: "GT"
date: "3 mai 2016"
output: html_document
---

## Titanic: Machine Learning from Disaster

### Datacamp tutorial

When the Titanic sank, 1502 of the 2224 passengers and crew got killed. One of the main reasons for 
this high level of casualties was the lack of lifeboats on this supposedly unsinkable ship.

Those that have seen the movie know that some individuals were more likely to survive the sinking 
(lucky Rose) than others (poor Jack). In this course you wil apply machine learning techniques to 
predict a passenger's chance of surviving using R.

Let's start with loading in the training and testing set into your R environment. You will use the 
training set to build your model, and the test set to validate it. The data is stored on the web as 
CSV files. You can load this data with the `read.csv()` function.

```{r}
train = read.csv("train.csv") # http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv
test = read.csv("test.csv") # http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv
```

Before starting with the actual analysis, it's important to understand the structure of your data. 
Both `test` and `train` are data frames, R's way of representing a dataset. You can easily explore a 
data frame using the `str()` function. `str()` gives you information such as the data types in the 
data frame (e.g. `int` for integer), the number of observations, and the number of variables.

```{r}
str(train)
str(test)
```

How many people in your training set survived the disaster with the Titanic? To see this, you can use 
the `table()` command in combination with the `$`-operator to select a single column of a data frame:

```{r}
# absolute numbers
table(train$Survived) 

# proportions
prop.table(table(train$Survived))
```

You see that 549 individuals died (62%) and 342 survived (38%). A simple prediction heuristic could thus 
be "majority wins": you predict every unseen observation to not survive.

In general, the `table()` command can help you to explore which variables have predictive value. 
For example, maybe gender could play a role as well? For a two-way comparison, also including gender, 
you can use

```{r}
table(train$Sex, train$Survived)
```

To get proportions, you can again wrap `prop.table()` around `table()`, but you'll have to specify whether 
you want row-wise or column-wise proportions: This is done by setting the second argument of `prop.table()`, called `margin`, to 1 or 2, respectively.

```{r}
prop.table(table(train$Sex, train$Survived), margin = 1)
```

Female had over a 50% chance of surviving.

Another variable that could influence survival is age: it's probable children were saved first. You can test 
this by creating a new column with a categorical variable `child`.

To add this new variable you need to do two things:

1. Create a new column `Child`, which is done through the `$` operator.
  
```{r}
train$Child <- NA
```
  
2. Provide the values for each observation (i.e., row) based on the age of the passenger. You can use a 
boolean test inside square brackets for this. Set the `Child` variable to `1` for passengers aged less
than 18 years old, and `0` the others.

```{r}
train$Child[train$Age < 18] <- 1
train$Child[train$Age >= 18] <- 0
```
  
3. Do a two-way comparison on the number of children vs adults that survived, in row-wise proportions

```{r}
prop.table(table(train$Child, train$Survived), margin = 1)
```

While less obviously than gender, Age also seems to have an impact on survival.

In your training set, females had over a 50% chance of surviving and males had less than a 50% 
chance of surviving. Hence, you could use this information for your first prediction: all females 
in the test set survive and all males in the test set die.

You use your test set for validating your predictions. You might have seen that, contrary to 
the training set, the test set has no `Survived` column. You add such a column using your 
predicted values. Next, when uploading your results, Kaggle will use this column (= your predictions) 
to score your performance.

```{r}
# Copy of test
test_one <- test

# Initialize a Survived column to 0
test_one$Survived = 0

# Set Survived to 1 if Sex equals "female"
test_one$Survived[test_one$Sex == "female"] = 1
```

Well done! If you want, you can already submit these first predictions to Kaggle by uploading
the `my_solution.csv` file.

Until now we did all the slicing and dicing ourself to find subjects that have a higher chance of 
surviving. A decision tree automates this process for you, and outputs a flowchart-like structure
that is easy to interpret.

Conceptually, the decision tree algorithm starts with all the data at the root node and scans all
the variables for the best one to split on. Once a variable is chosen, you do the split and go
down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known
as terminal nodes, and the majority vote of the observations in that node determine how to predict
for new observations thant end up in that terminal node.

To create your first decision tree, you'll make use of R's `rpart` package. Instead of needing to
writing an algo yourself you can use this package to build a decision tree.

```{r}
# Load in the R package
library(rpart)

```

OK, your package is loaded now.

Inside `rpart`, there is the `rpart()` function to build your first decision tree. The function
takes multiple arguments:

- `formula`: specifying variable of interest, and the variables used for prediction 
  (e.g. `formula = Survived ~ Sex + Age`).
- `data`; The data set to build the decision tree (here `train`).
- `method`: Type of prediction you want. We want to predict a categorical variable, so
  classification: `method = "class"`.

Your call could look like this:

```
my_tree <- rpart(Survived ~ Sex + Age,
                 data = train,
                 method = "class")
```

To visualize the resulting tree, you can use the `plot(my_tree)` and `text(my_tree)`.
The resulting graphs will not be that informative, but R has packages to make it all
fancier: `rattle`, `rpart.plot`, and `RColorBrewer`.

```{r}
# Build the decision tree, you want to predict Survived based on Pclass, Sex, Age,
# SibSp, Parch, Fare and Embarked.
# Use the train data to build the tree
# Use method to specify that you want to classify
my_tree_two <- rpart(Survived ~ Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = "class")

# Visualize my_tree_two with plot() and text()
plot(my_tree_two)
text(my_tree_two)

# Load the R packages rattle, rpart.plot and RColorBrewer
library(rattle)
library(rpart.plot)
library(RColorBrewer)

# Use fancyRpartPlot(my_tree) to create a much fancier visualization of your tree
fancyRpartPlot(my_tree_two)
```

Look at the decision tree you just created. It's a very clear graph, that is easy
to read and to interpret. Also, you see that thanks to the algorithm we can easily
take into account more variables as opposed to creating the segments manually.

Based on your decision tree, what variables play the most important role to determine
whether or not a passenger will survive?

`Sex`, `Age`, `Pclass`, `SibSp`, `Parch` and `Fare`

To send a submission to Kaggle you need to predict the survival rates for the 
observations in the test set. Previosly we created rather amateuristic predictions
with manual subsetting operations. Now that we have a decsion tree, we can
make use of the `predict()` function to "generate" our answer:

`predict(my_tree_two, test, type = "class")`

Here, `my_tree_two` is the tree model you've just built, `test` is the data
set to build the predictions for, and `type = "class"` specifies that you
want to classify observations.

Before you can submit to Kaggle, you'll have to convert your predictions to
a CSV file with exactly 418 entries and 2 columns `PassengerId` and `Survived`.
Head over to the instructions to get to it!

```{r}
# Use predict() as specified above to make predictions on the test set. Assign
# the result to my_prediction

my_prediction <- predict(my_tree_two, test, type = "class")

# Create the my_solution data frame that is in line with Kaggle's standard:
# the PassengerId column should contain the PassengerId column of test
# the Survived column should contain the values in my_prediction

my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)

# Check that my_solution has 418 entries with nrow()

nrow(my_solution)

# Write the data in my_solution to "my_solution.csv" using the write.csv() 
# Don't remove the row.names = FALSE argument.

write.csv(my_solution, file = "my_solution1.csv", row.names = FALSE)
```

If you submitted the solution of the previous exercise, you got a result that
outperform a solution using purely gender. Hurray!

Maybe we ca improve even more by making a more complex model? In `rpart`, the 
amount of detail is defined by two parameters:

- `cp` determines when the splitting up of the decision tree stops.
- `minsplit` determines the minimum amount of observations in a leaf of the tree

We can create a  `super_model` using `cp = 0` (no stopping of splits) and 
`minsplit = 2` (smallest leaf possible). This will create the best model!
Or not?

```{r}
super_model <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                     data = train, 
                     method = "class", 
                     control = rpart.control(minsplit = 2, cp = 0))

### Check out the resulting plot with:

fancyRpartPlot(super_model)
```

Looking complex, but using this model to make predictions won't give you a 
good score on Kaggle. Why? Because you created very specific rules based on 
the data in the training set. These very detailed rules are only relevant for
the training set but cannot be generalized to unknown set. 
_You overfitted your tree_. Always be aware of this danger!


Change the command that builds `super_model`:

- Call the resulting tree `my_tree_three`
- Use the same `formula`, `data` and `method`
- Set `minsplit` to `50` and `cp` to `0`.

Visualize `my_tree_three` with `fancyRpartPlot()`.

```{r}
my_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                     data = train, 
                     method = "class", 
                     control = rpart.control(minsplit = 50, cp = 0))
fancyRpartPlot(my_tree_three)
```

You always want a tree that generalizes well to unseen data.

Data Science is an art that benefits from a human element. Enter feature engineering:
creatively engineering your own features by combining the different existing variables.

While feature engineering is a discipline in itself, too broad to be covered here
in detail, let's have a look at a simple example and create a new predictive attribute:
`family_size`.

A valid assumption is that larger families need more time to get together on sinking
ship, and hence have less chance of surviving. Family size is determined by the 
variable `SibSp` and `Parch`, which indicate the number of family members a certain
passenger is travelling with. So when doing feature engineering, you add a new
variable `family_size`, which is the sum of `SibSp` and `Parch` plus one (the observation
itself), to the test and train set.

- Create a new train set `train_two` that differs from `train` only by having an
  extra column with your feature engineered variable `family_size`.

```{r}
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1

# Build my_tree_four including family_size with train_two data
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size, 
                      data = train_two, method = "class")

# Visualize the new decision tree with fancyRpartPlot()
fancyRpartPlot(my_tree_four)
```

If you have a close look at the decision tree obtained you see that
`family_size` is not included. Apparently other variables played a more 
important role. This is part of the game as well. Sometimes things will not
turn out as expected, but it is here that you can make the difference.

Was it coincidence that upper-class Rose survived and third-class passenger
Jack not? Let's have a look...

You have access to a new train and test set named `train_new` and `test_new`.

```{r}
train_new <- read.csv("train_new.csv")
test_new <- read.csv("test_new.csv")
```

These data sets contain a new column with the name `Title` (referring to Miss, Mr). 
`Title` is another example of feature engineering: it's a new variable that possibly 
improves the model.

- Build e new decsion tree `my_tree_five`: make sure to include the `Title`
  variable, and to create the tree based on `train_new`
- Visualize `my_tree_five` with fancyRpartPlot().

```{r}
my_tree_five <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title, 
                      data = train_new, method = "class")

# Visualize my_tree_five
fancyRpartPlot(my_tree_five)
```

Notice that `Title` appears in one of the nodes.

- Build the `predict()` call to create `my_prediction1`: the function should use
  `my_tree_five` and `test_new` to make predictions
- Then create the data frame `my_solution2` and write it to a CSV file. 


```{r}
# Make prediction
my_prediction1 <- predict(my_tree_five, test_new, type = "class")

# Make results ready for submission
my_solution2 <- data.frame(PassengerId = test_new$PassengerId, Survived = my_prediction1)
write.csv(my_solution2, file = "my_solution2.csv", row.names = FALSE)
```

These steps make the solution ready for a submission on Kaggle.

A detailed study of Random Forests would take this tutorial a bit too far. However,
since it's an often used machine learning technique, a general understanding and an 
illustration in R won't hurt.

In layman's terms, the Random Forest technique handles the overfitting problem you faced
with decision trees. It grows multiple (very deep) classification trees using the training
set. At the time of prediction, each tree is used to come up with a prediction and every 
outcome is counted as a vote. For example, if you have trained 3 trees with 2 saying
a passenger in the test set will survive and 1 says he will not, the passenger will
be classified as a survivor. This approach of overtraining trees, but having the 
majority's vote count as the actual classification decision, avoids overfitting.

Before starting with the actual analysis, you first need to meet one big condition
of Random Forests: no missing values in your data frame. Let's get to work.

You'll find here the code to clean the entire dataset from missing data and 
split it up in training and test.

```{r}
# Read all data, both training and test set
all_data <- read.csv("all_data.csv")

# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passenger embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"

# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)

# Passenger on row 1044 has an NA Fare value. Let's replace it with the median
# fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm=TRUE)

# How to fill in missing Age values ?
# We make a prediction of a passengers Age using the other variables
# an a decision tree model. This time you give method = "anova" since
# since you're predicting a continous variable.
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + 
                         Title + family_size,
                       data = all_data[!is.na(all_data$Age),], 
                       method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])

# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
```

Great, your data is cleaned up. One more important element in Random Forest is 
randomization to avoid the creation of the same tree from the training set.
You randomize in two ways: by taking a randomized sample of the rows in your
training set and by only working with a limited and changing number of the
available variables for every node of the tree.

